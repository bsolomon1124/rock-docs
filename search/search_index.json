{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 RockNSM is the premier sensor platform for Network Security Monitoring (NSM) hunting and incident response (IR) operations. ROCK is the open-source security distribution that prioritizes being: Reliable Scalable Secure Above all else, ROCK exists to aid the analyst in the fight to find the adversary. Quickstart \u00b6 If you're already familiar with building sensors you can jump straight into things in the Quickstart Guide . Latest \u00b6 See the Releases page for the latest info on ROCK 2.5. Contents \u00b6 About - project overview / background / dataflow Install - requirements / install media / installation Configure - configuring for your use case Deploy - development via Ansible playbooks Usage - basic usage overview and troubleshooting Services - component directory and management info Reference - concept / design, components / dataflow","title":"Welcome"},{"location":"#welcome","text":"RockNSM is the premier sensor platform for Network Security Monitoring (NSM) hunting and incident response (IR) operations. ROCK is the open-source security distribution that prioritizes being: Reliable Scalable Secure Above all else, ROCK exists to aid the analyst in the fight to find the adversary.","title":"Welcome"},{"location":"#quickstart","text":"If you're already familiar with building sensors you can jump straight into things in the Quickstart Guide .","title":"Quickstart"},{"location":"#latest","text":"See the Releases page for the latest info on ROCK 2.5.","title":"Latest"},{"location":"#contents","text":"About - project overview / background / dataflow Install - requirements / install media / installation Configure - configuring for your use case Deploy - development via Ansible playbooks Usage - basic usage overview and troubleshooting Services - component directory and management info Reference - concept / design, components / dataflow","title":"Contents"},{"location":"quickstart/","text":"ROCK Quickstart \u00b6 This is a hasty guide to get right into building your very own sensor, just for users already familiar with building sensors and know what they're doing. If you're not an NSM ninja, you can start building from the beginning of the full docs here . Getting the Bits \u00b6 Download the lastest ROCK build from download.rocknsm.io and create a bootable disk using your favorite burning utility. Install Rock \u00b6 If you have a network connection available, plug it into the management port Select the Custom Install at initial boot Configure disk partitions to suit your needs Create an administrative user Reboot and log back in as admin user Configure & Deploy ROCK \u00b6 Familiarize with the updated ROCK commands / options by running $ rock Run $ sudo rock setup Follow each step of the setup TUI by the numbers Deploy your sensor by executing the last setup option \"Run Installer\" Next Up \u00b6 A good next step would be the usage section for details on doing basic functions checks before plugging into the stream.","title":"ROCK Quickstart"},{"location":"quickstart/#rock-quickstart","text":"This is a hasty guide to get right into building your very own sensor, just for users already familiar with building sensors and know what they're doing. If you're not an NSM ninja, you can start building from the beginning of the full docs here .","title":"ROCK Quickstart"},{"location":"quickstart/#getting-the-bits","text":"Download the lastest ROCK build from download.rocknsm.io and create a bootable disk using your favorite burning utility.","title":"Getting the Bits"},{"location":"quickstart/#install-rock","text":"If you have a network connection available, plug it into the management port Select the Custom Install at initial boot Configure disk partitions to suit your needs Create an administrative user Reboot and log back in as admin user","title":"Install Rock"},{"location":"quickstart/#configure-deploy-rock","text":"Familiarize with the updated ROCK commands / options by running $ rock Run $ sudo rock setup Follow each step of the setup TUI by the numbers Deploy your sensor by executing the last setup option \"Run Installer\"","title":"Configure &amp; Deploy ROCK"},{"location":"quickstart/#next-up","text":"A good next step would be the usage section for details on doing basic functions checks before plugging into the stream.","title":"Next Up"},{"location":"about/backstory/","text":"Backstory \u00b6 ROCK is a tool that was created to solve a problem that was realized several years ago while engaged in real-world missions and training exercises. That problem was that the pervasive network sensor platform at the time, had many architectural and operational issues. It was built on an insecure platform, had performance problems, and did not follow the Unix Philosophy . The origin of RockNSM can be traced back to the Fall of 2014 when a couple of wide-eyed dreamers started working on their own solution while drinking whiskey in a hotel room. The project developed from an on-mission hasty replacement, to an all-in-one sensor solution, and now a more full featured analysis stack capable of multi-node deployments. Credit \u00b6 This project is made possible by the efforts of an ever-growing list of amazing people. Take a look around our project to see all our contributors.","title":"Backstory"},{"location":"about/backstory/#backstory","text":"ROCK is a tool that was created to solve a problem that was realized several years ago while engaged in real-world missions and training exercises. That problem was that the pervasive network sensor platform at the time, had many architectural and operational issues. It was built on an insecure platform, had performance problems, and did not follow the Unix Philosophy . The origin of RockNSM can be traced back to the Fall of 2014 when a couple of wide-eyed dreamers started working on their own solution while drinking whiskey in a hotel room. The project developed from an on-mission hasty replacement, to an all-in-one sensor solution, and now a more full featured analysis stack capable of multi-node deployments.","title":"Backstory"},{"location":"about/backstory/#credit","text":"This project is made possible by the efforts of an ever-growing list of amazing people. Take a look around our project to see all our contributors.","title":"Credit"},{"location":"about/dataflow/","text":"Data Flow \u00b6 This is a high level model of how packets flow through the sensor:","title":"Data Flow"},{"location":"about/dataflow/#data-flow","text":"This is a high level model of how packets flow through the sensor:","title":"Data Flow"},{"location":"about/what_is_it/","text":"What is ROCK \u00b6 The Mission \u00b6 Reliable - we believe the folks at Red Hat do Linux right. ROCK is built on Centos7 and provides an easy path to a supported enterprise OS ( RHEL ). Secure - with SELinux, ROCK is highly secure by default. SELinux uses context to define security controls to prevent, for instance, a text editor process from talking to the internet. #setenforce1 Scalable - Whether you're tapping a SoHo network or a large enterprise, ROCK is designed with scale in mind. Capability \u00b6 Passive and reliable high-speed data acquisition via AF_PACKET, feeding systems for metadata (Bro), signature detection (Suricata), extracted network file metadata (FSF), and full packet capture (Stenographer). A messaging layer (Kafka and Logstash) that provides flexibility in scaling the platform to meet operational needs, as well as providing some degree of data reliability in transit. Reliable data storage and indexing (Elasticsearch) to support rapid retrieval and analysis (Kibana and Docket) of the data. Pivoting off Kibana data rapidly into full packet capture (Docket and Stenographer). Components \u00b6 Full Packet Capture via Google Stenographer Protocol Analysis and Metadata via Bro Signature Based Alerting via Suricata Recursive File Scanning via FSF . Output from Suricata and FSF are moved to message queue via Filebeat Message Queuing and Distribution via Apache Kafka Message Transport via Logstash Data Storage, Indexing, and Search via Elasticsearch Analyst Toolkit \u00b6 Kibana provides data UI and visualization Docket allows for quick and targeted pivoting to PCAP","title":"What is ROCK"},{"location":"about/what_is_it/#what-is-rock","text":"","title":"What is ROCK"},{"location":"about/what_is_it/#the-mission","text":"Reliable - we believe the folks at Red Hat do Linux right. ROCK is built on Centos7 and provides an easy path to a supported enterprise OS ( RHEL ). Secure - with SELinux, ROCK is highly secure by default. SELinux uses context to define security controls to prevent, for instance, a text editor process from talking to the internet. #setenforce1 Scalable - Whether you're tapping a SoHo network or a large enterprise, ROCK is designed with scale in mind.","title":"The Mission"},{"location":"about/what_is_it/#capability","text":"Passive and reliable high-speed data acquisition via AF_PACKET, feeding systems for metadata (Bro), signature detection (Suricata), extracted network file metadata (FSF), and full packet capture (Stenographer). A messaging layer (Kafka and Logstash) that provides flexibility in scaling the platform to meet operational needs, as well as providing some degree of data reliability in transit. Reliable data storage and indexing (Elasticsearch) to support rapid retrieval and analysis (Kibana and Docket) of the data. Pivoting off Kibana data rapidly into full packet capture (Docket and Stenographer).","title":"Capability"},{"location":"about/what_is_it/#components","text":"Full Packet Capture via Google Stenographer Protocol Analysis and Metadata via Bro Signature Based Alerting via Suricata Recursive File Scanning via FSF . Output from Suricata and FSF are moved to message queue via Filebeat Message Queuing and Distribution via Apache Kafka Message Transport via Logstash Data Storage, Indexing, and Search via Elasticsearch","title":"Components"},{"location":"about/what_is_it/#analyst-toolkit","text":"Kibana provides data UI and visualization Docket allows for quick and targeted pivoting to PCAP","title":"Analyst Toolkit"},{"location":"configure/reference/","text":"The primary configuration file for RockNSM is found at /etc/rocknsm/config.yml . This file defines key information that drives the Ansible deployment playbook like network interface setup, cpu cores assignment, and much more. There are a lot of options to tune here so take time to familiarize. A template of this file in it's entirety can be found [here on github] , but for greater clarity let's break it down into it's major sections: Network Interface \u00b6 As mentioned previously, ROCK takes the interface with an ip address / gateway and will use that as the management NIC. config.yml displays the remaining interfaces that will be used to MONITOR traffic. Let's run through a basic example: [admin@rock ~]$ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether ... inet 192.168.1.207/24 brd 192.168.1.255 scope global noprefixroute dynamic enp0s3 ... 3: enp0s4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether ... The demo box above has 2 NICs: 1. enp0s3 - is plugged in for install and deployment with an ip address from local dhcp. This will be used to manage the sensor 2. enp0s4 - will be unused (not connected) during install and deployment and be listed as a rock_monif in the config file The config file shows the other interface ( enp0s3 ) is listed as MONITOR interface. # interfaces that should be configured for sensor applications rock_monifs: - enp0s3 Sensor Resource \u00b6 # Set the hostname of the sensor: rock_hostname: rocknsm_sensor_1 # Set the Fully Qualified Domain Name: rock_fqdn: rocknsm_sensor_1.rocknsm.lan # Set the number of CPUs assigned to Bro: bro_cpu: 2 # Set the Elasticsearch cluster name: es_cluster_name: rocknsm # Set the Elasticsearch cluster node name: es_node_name: localhost # Set the value of Elasticsearch memory: es_mem: 5 Installation Source \u00b6 We've taken into consideration that your sensor won't always have internet access. Currently the default value is set to rock_online_install: True : # The primary installation variable defines the ROCK installation method: # ONLINE: used if the system may reach out to the internet # OFFLINE: used if the system may *NOT* reach out to the internet # The default value \"False\" will deploy using OFFLINE (local) repos. # A value of \"True\" will perform an install using ONLINE mirrors. rock_online_install: True Online \u00b6 Does your sensor has access to upstream online repositories? If so, then make sure that this value is set to rock_online_install: True . Offline \u00b6 If you are in an offline environment, then set it to rock_online_install: False . Ansible will deploy using the locally cached files found in /srv/rocknsm . Note: In our next release the default behavior will be changed to an offline install (reference Issue #376 ) Data Retention \u00b6 This section controls how long NSM data stay on the sensor: # Set the interval in which Elasticsearch indexes are closed: elastic_close_interval: 15 # Set the interval in which Elasticsearch indexes are deleted: elastic_delete_interval: 60 # Set value for Kafka retention (in hours): kafka_retention: 168 # Set value for Bro log retention (in days): bro_log_retention: 0 # Set value for Bro statistics log retention (in days): bro_stats_retention: 0 # Set how often logrotate will roll Suricata log (in days): suricata_retention: 3 # Set value for FSF log retention (in days): fsf_retention: 3 Component Options \u00b6 This is a critical section that provides boolean options to choose what components of ROCK are installed and enabled during deployment. rock_services: - name: bro quota_weight: 1 installed: True enabled: True - name: stenographer quota_weight: 8 installed: True enabled: True - name: docket quota_weight: 0 installed: True enabled: True - name: suricata quota_weight: 2 installed: True enabled: True - name: elasticsearch quota_weight: 4 installed: True enabled: True - name: kibana quota_weight: 0 installed: True enabled: True - name: zookeeper quota_weight: 0 installed: True enabled: True - name: kafka quota_weight: 4 installed: True enabled: True - name: lighttpd quota_weight: 0 installed: True enabled: True - name: fsf quota_weight: 1 installed: True enabled: True - name: filebeat quota_weight: 0 installed: True enabled: True A good example for changing this section would involve Stenographer . Collecting raw PCAP is resource and storage intensive . You're machine may not be able to handle that and if you just wanted to focus on network logs, then you would set both options in the config file to disable installing and enabling Stenographer.","title":"Config Reference"},{"location":"configure/reference/#network-interface","text":"As mentioned previously, ROCK takes the interface with an ip address / gateway and will use that as the management NIC. config.yml displays the remaining interfaces that will be used to MONITOR traffic. Let's run through a basic example: [admin@rock ~]$ ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether ... inet 192.168.1.207/24 brd 192.168.1.255 scope global noprefixroute dynamic enp0s3 ... 3: enp0s4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether ... The demo box above has 2 NICs: 1. enp0s3 - is plugged in for install and deployment with an ip address from local dhcp. This will be used to manage the sensor 2. enp0s4 - will be unused (not connected) during install and deployment and be listed as a rock_monif in the config file The config file shows the other interface ( enp0s3 ) is listed as MONITOR interface. # interfaces that should be configured for sensor applications rock_monifs: - enp0s3","title":"Network Interface"},{"location":"configure/reference/#sensor-resource","text":"# Set the hostname of the sensor: rock_hostname: rocknsm_sensor_1 # Set the Fully Qualified Domain Name: rock_fqdn: rocknsm_sensor_1.rocknsm.lan # Set the number of CPUs assigned to Bro: bro_cpu: 2 # Set the Elasticsearch cluster name: es_cluster_name: rocknsm # Set the Elasticsearch cluster node name: es_node_name: localhost # Set the value of Elasticsearch memory: es_mem: 5","title":"Sensor Resource"},{"location":"configure/reference/#installation-source","text":"We've taken into consideration that your sensor won't always have internet access. Currently the default value is set to rock_online_install: True : # The primary installation variable defines the ROCK installation method: # ONLINE: used if the system may reach out to the internet # OFFLINE: used if the system may *NOT* reach out to the internet # The default value \"False\" will deploy using OFFLINE (local) repos. # A value of \"True\" will perform an install using ONLINE mirrors. rock_online_install: True","title":"Installation Source"},{"location":"configure/reference/#online","text":"Does your sensor has access to upstream online repositories? If so, then make sure that this value is set to rock_online_install: True .","title":"Online"},{"location":"configure/reference/#offline","text":"If you are in an offline environment, then set it to rock_online_install: False . Ansible will deploy using the locally cached files found in /srv/rocknsm . Note: In our next release the default behavior will be changed to an offline install (reference Issue #376 )","title":"Offline"},{"location":"configure/reference/#data-retention","text":"This section controls how long NSM data stay on the sensor: # Set the interval in which Elasticsearch indexes are closed: elastic_close_interval: 15 # Set the interval in which Elasticsearch indexes are deleted: elastic_delete_interval: 60 # Set value for Kafka retention (in hours): kafka_retention: 168 # Set value for Bro log retention (in days): bro_log_retention: 0 # Set value for Bro statistics log retention (in days): bro_stats_retention: 0 # Set how often logrotate will roll Suricata log (in days): suricata_retention: 3 # Set value for FSF log retention (in days): fsf_retention: 3","title":"Data Retention"},{"location":"configure/reference/#component-options","text":"This is a critical section that provides boolean options to choose what components of ROCK are installed and enabled during deployment. rock_services: - name: bro quota_weight: 1 installed: True enabled: True - name: stenographer quota_weight: 8 installed: True enabled: True - name: docket quota_weight: 0 installed: True enabled: True - name: suricata quota_weight: 2 installed: True enabled: True - name: elasticsearch quota_weight: 4 installed: True enabled: True - name: kibana quota_weight: 0 installed: True enabled: True - name: zookeeper quota_weight: 0 installed: True enabled: True - name: kafka quota_weight: 4 installed: True enabled: True - name: lighttpd quota_weight: 0 installed: True enabled: True - name: fsf quota_weight: 1 installed: True enabled: True - name: filebeat quota_weight: 0 installed: True enabled: True A good example for changing this section would involve Stenographer . Collecting raw PCAP is resource and storage intensive . You're machine may not be able to handle that and if you just wanted to focus on network logs, then you would set both options in the config file to disable installing and enabling Stenographer.","title":"Component Options"},{"location":"configure/rock-manager/","text":"One of the new features in version 2.4 is the Rock Manager. This script provides a one stop shop for managing all things ROCK. To display all available commands and options run: $ rock Usage: /usr/sbin/rock COMMAND [options] Commands: setup Launch TUI to configure this host for deployment tui Alias for setup ssh-config Configure hosts in inventory to use key-based auth (multinode) deploy Deploy selected ROCK components deploy-offline Same as deploy --offline (Default ISO behavior) deploy-online Same as deploy --online stop Stop all ROCK services start Start all ROCK services restart Restart all ROCK services status Report status for all ROCK services genconfig Generate default configuration based on current system destroy Destroy all ROCK data: indexes, logs, PCAP, i.e. EVERYTHING NOTE: Will not remove any services, just the data Options: --config, -c <config_yaml> Specify full path to configuration overrides --extra, -e <ansible variables> Set additional variables as key=value or YAML/JSON passed to ansible-playbook --help, -h Show this usage information --inventory, -i <inventory_path> Specify path to Ansible inventory file --limit <host> Specify host to run plays --list-hosts Outputs a list of matching hosts; does not execute anything else --list-tags List all available tags --list-tasks List all tasks that would be executed --offline, -o Deploy ROCK using only local repos (Default ISO behavior) --online, -O Deploy ROCK using online repos --playbook, -p <playbook_path> Specify path to Ansible playbook file --skip-tags <tags> Only run plays and tasks whose tags do not match these values --tags, -t <tags> Only run plays and tasks tagged with these values --verbose, -v Increase verbosity of ansible-playbook As you can see above, the rock command has many options available, so here's a basic breakdown of how you would use this command: Initial Deployment \u00b6 sudo rock setup # launches interface to configure settings sudo rock ssh-setup # configures ssh access to rock nodes (multinode) sudo rock deploy-offline # deploys ROCK using local pre-staged packages sudo rock deploy-online # deploys ROCK using online repo packages Stand by for a detailed walkthrough of the initial deployment process in the \"Deploy\" section coming up. Basic Usage \u00b6 sudo rock status # display status of all ROCK services sudo rock stop # stop all ROCK services sudo rock start # start all ROCK services sudo rock restart # restart all ROCK services Advanced Usage \u00b6 sudo rock genconfig # regenerate a fresh config.yml file sudo rock destroy # removes ALL sensor data (logs, indices, PCAP)","title":"Rock Manager"},{"location":"configure/rock-manager/#initial-deployment","text":"sudo rock setup # launches interface to configure settings sudo rock ssh-setup # configures ssh access to rock nodes (multinode) sudo rock deploy-offline # deploys ROCK using local pre-staged packages sudo rock deploy-online # deploys ROCK using online repo packages Stand by for a detailed walkthrough of the initial deployment process in the \"Deploy\" section coming up.","title":"Initial Deployment"},{"location":"configure/rock-manager/#basic-usage","text":"sudo rock status # display status of all ROCK services sudo rock stop # stop all ROCK services sudo rock start # start all ROCK services sudo rock restart # restart all ROCK services","title":"Basic Usage"},{"location":"configure/rock-manager/#advanced-usage","text":"sudo rock genconfig # regenerate a fresh config.yml file sudo rock destroy # removes ALL sensor data (logs, indices, PCAP)","title":"Advanced Usage"},{"location":"configure/setup-tui/","text":"Overview \u00b6 Another new 2.4 feature is the Setup ( T )ext ( U )ser ( I )nterface. This allows for a unified interface in order to configure ROCK sensor settings. The new interface can be called by running either: sudo rock setup sudo rock tui The Setup TUI currently only supports configuring a deploying a single-node instance of ROCK. Building Manually \u00b6 For the ROCK veterans in the house, you can still run a manual build by: customizing your config file at: /etc/rocknsm/config.yml deploy with the new rock manager using preferred packages: sudo rock deploy-offline sudo rock deploy-online","title":"Setup TUI"},{"location":"configure/setup-tui/#overview","text":"Another new 2.4 feature is the Setup ( T )ext ( U )ser ( I )nterface. This allows for a unified interface in order to configure ROCK sensor settings. The new interface can be called by running either: sudo rock setup sudo rock tui The Setup TUI currently only supports configuring a deploying a single-node instance of ROCK.","title":"Overview"},{"location":"configure/setup-tui/#building-manually","text":"For the ROCK veterans in the house, you can still run a manual build by: customizing your config file at: /etc/rocknsm/config.yml deploy with the new rock manager using preferred packages: sudo rock deploy-offline sudo rock deploy-online","title":"Building Manually"},{"location":"deploy/initial-access/","text":"We strive to do the little things right, so rather than having Kibana available to everyone in the free world, it's sitting behind a reverse proxy and secured by an (XKCD) Passphrase . The credentials are written to the home directory of the user that runs the deploy script. Most of the time, this will be the administrative user created at installation e.g. /home/admin . Kibana \u00b6 To get into the Kibana interface: Note: we are aware of a new change with macOS Catalina and the Chrome browser that requires SSL certificates are signed with a CA. We're working on the issue and will update the SSL proxy process soon. Until then, Safari and Firefox work. Windows is not affected. Copy the passphrase in ~/KIBANA_CREDS.README Point your browser to Kibana: https://<SENSORIP> https://<HOSTNAME> (if you have DNS set up) Enter the user and password Profit!","title":"Initial Access"},{"location":"deploy/initial-access/#kibana","text":"To get into the Kibana interface: Note: we are aware of a new change with macOS Catalina and the Chrome browser that requires SSL certificates are signed with a CA. We're working on the issue and will update the SSL proxy process soon. Until then, Safari and Firefox work. Windows is not affected. Copy the passphrase in ~/KIBANA_CREDS.README Point your browser to Kibana: https://<SENSORIP> https://<HOSTNAME> (if you have DNS set up) Enter the user and password Profit!","title":"Kibana"},{"location":"deploy/multi-node/","text":"Now let's look at how to perform a ROCK deployment across multiple sensors. This is where we can break out server roles for more complex and distributed environment. Assumptions \u00b6 This document assumes that you have done a fresh installation of ROCK on all ROCK nodes using the latest ISO . 3 (minimum) sensors sensors on same network, able to communicate in this demo there are 3 newly installed ROCK sensors: rock01.rock.lan - 172.16.1.23X rock02.rock.lan - 172.16.1.23X rock03.rock.lan - 172.16.1.23X admin account created at install (same username for all nodes) Multi-node Configuration \u00b6 Designate Deployment Manager \u00b6 First and foremost, one sensor will need to be designated as the \"Deployment Manager\" (referred to as \"DM\" for the rest of this guide). This is a notional designation to keep the process clean and as simple as possible. For our lab we will use rock01 as the DM. NOTE: all following commands will be run from your Deployment Manager (DM) box Confirm that you can remotely connect to the DM: ssh admin@<DM-ip> Confirm that you can ping all other rock hosts. For this example: ping 172.16.1.236 #localhost ping 172.16.1.237 ping 172.16.1.239 Edit Inventory File \u00b6 A deployment is performed using an Ansible playbooks, and target systems are defined in an Ansible inventory file . In ROCK this file is located at /etc/rocknsm/hosts.ini . We need to add additional ROCK sensors to the following sections of the inventory file: [rock] [web] [zookeeper] [es_masters] [es_data] [es_ingest] [rock] \u00b6 This group defines what nodes will be running ROCK services (Zeek, Suricata, Stenographer, Kafka, Zookeeper) example: [rock] rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239 [web] \u00b6 This group defines what node will be running web services (Kibana, Lighttpd, and Docket). example: [web] rock03.rock.lan ansible_host=172.16.1.239 [zookeeper] \u00b6 This group defines what node(s) will be running zookeeper (Kafka cluster manager). example: [zookeeper] rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local [es_masters] \u00b6 This group defines what node(s) will be running Elasticsearch, acting as master nodes. example: [es_masters] rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239 [es_data] \u00b6 This group defines what node(s) will be running Elasticsearch, acting as data nodes. example: [es_data] rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239 [es_ingest] \u00b6 This group defines what node(s) will be running Elasticsearch, acting as ingest nodes. For ROCK deployments, generally, everything that is data node eligible is also ingest node eligible. example: [es_ingest] rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239 Example Inventory \u00b6 Above, we broke out every section. Let's take a look at a cumulative example of all the above sections in host.ini file for this demo: Simple example inventory: [rock] rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239 [web] rock03.rock.lan ansible_host=172.16.1.239 [lighttpd:children] web [sensors:children] rock [bro:children] sensors [fsf:children] sensors [kafka:children] sensors [stenographer:children] sensors [suricata:children] sensors [filebeat:children] fsf suricata [zookeeper] rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local [elasticsearch:children] es_masters es_data es_ingest [es_masters] # This group should only ever contain exactly 1 or 3 nodes! #simplerockbuild.simplerock.lan ansible_host=127.0.0.1 ansible_connection=local # Multi-node example # #elasticsearch0[1:3].simplerock.lan rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239 [es_data] #simplerockbuild.simplerock.lan ansible_host=127.0.0.1 ansible_connection=local # Multi-node example # #elasticsearch0[1:4].simplerock.lan rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239 [es_ingest] #simplerockbuild.simplerock.lan ansible_host=127.0.0.1 ansible_connection=local # Multi-node example # #elasticsearch0[1:4].simplerock.lan rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239 [elasticsearch:vars] # Disable all node roles by default node_master=false node_data=false node_ingest=false [es_masters:vars] node_master=true [es_data:vars] node_data=true [es_ingest:vars] node_ingest=true [docket:children] web [kibana:children] web [logstash:children] sensors If you have a bit of a more complex use-case, here's a more extensive example that has: 3 Elasticsearch master nodes 4 Elasticsearch data nodes 1 Elasticsearch coordinating node 1 Logstash node 2 ROCK sensors We also added a few more sections, [Logstash] and [Elasticsearch] . We did this because we're breaking Logstash out into it's own server, as well as adding a coordinating node that will just be part of Elasticsearch, but not a data, ingest, or master node. # Define all of our Ansible_hosts up front es-master-0.rock.lan ansible_host=192.168.1.5 es-master-1.rock.lan ansible_host=192.168.1.6 es-master-2.rock.lan ansible_host=192.168.1.7 es-data-0.rock.lan ansible_host=192.168.1.8 es-data-1.rock.lan ansible_host=192.168.1.9 es-data-2.rock.lan ansible_host=192.168.1.10 es-data-3.rock.lan ansible_host=192.168.1.11 es-coord-0.rock.lan ansible_host=192.168.1.12 ls-pipeline-0.rock.lan ansible_host=192.168.1.13 rock-0.rock.lan ansible_host=192.168.1.14 rock-1.rock.lan ansible_host=192.168.1.15 [rock] rock-0.rock.lan rock-1.rock.lan [web] es-coord-0.rock.lan [lighttpd:children] web [sensors:children] rock [zeek:children] sensors [fsf:children] sensors [kafka:children] sensors [stenographer:children] sensors [suricata:children] sensors [filebeat:children] fsf suricata [zookeeper] rock-0.rock.lan [elasticsearch:children] es_masters es_data es_ingest [elasticsearch] es-coord-0.rock.lan [es_masters] # This group should only ever contain exactly 1 or 3 nodes! # Multi-node example # #elasticsearch0[1:3].simplerock.lan es-master-[0:2].rock.lan [es_data] # Multi-node example # #elasticsearch0[1:4].simplerock.lan es-data-[0:3].rock.lan [es_ingest] # Multi-node example # #elasticsearch0[1:4].simplerock.lan es-data-[0:3].rock.lan [elasticsearch:vars] # Disable all node roles by default node_master=false node_data=false node_ingest=false [es_masters:vars] node_master=true [es_data:vars] node_data=true [es_ingest:vars] node_ingest=true [docket:children] web [kibana:children] web [logstash] ls-pipeline-0.rock.lan SSH Config \u00b6 After the inventory file entries are finalized, it's time to configure ssh in order for Ansible to communicate to all nodes during deployment. The ssh-config command will perform the following on all other nodes in the inventory: add an authorized keys entry add the user created at install to the sudoers file To configure ssh run: sudo rock ssh-config Update ROCK Configuration File \u00b6 There are a few last steps, both are in /etc/rocknsm/config.yml If you are doing an offline installation, you need to change rock_online_install: True to rock_online_install: False . If you are going to have 2 different sensor components, you need to manually define your Zookeeper host by adding kafka_zookeeper_host: Zookeeper IP to the configuration file. It isn't necessary where it is placed. Multi-node Deployment \u00b6 Now, we're finally ready to deploy! sudo rock deploy Success \u00b6 Once the deployment is completed with the components you chose, you'll be congratulated with a success banner. Congratulations! We strongly recommend giving all of the services a fresh stop/start with sudo rockctl restart .","title":"Multi Node"},{"location":"deploy/multi-node/#assumptions","text":"This document assumes that you have done a fresh installation of ROCK on all ROCK nodes using the latest ISO . 3 (minimum) sensors sensors on same network, able to communicate in this demo there are 3 newly installed ROCK sensors: rock01.rock.lan - 172.16.1.23X rock02.rock.lan - 172.16.1.23X rock03.rock.lan - 172.16.1.23X admin account created at install (same username for all nodes)","title":"Assumptions"},{"location":"deploy/multi-node/#multi-node-configuration","text":"","title":"Multi-node Configuration"},{"location":"deploy/multi-node/#designate-deployment-manager","text":"First and foremost, one sensor will need to be designated as the \"Deployment Manager\" (referred to as \"DM\" for the rest of this guide). This is a notional designation to keep the process clean and as simple as possible. For our lab we will use rock01 as the DM. NOTE: all following commands will be run from your Deployment Manager (DM) box Confirm that you can remotely connect to the DM: ssh admin@<DM-ip> Confirm that you can ping all other rock hosts. For this example: ping 172.16.1.236 #localhost ping 172.16.1.237 ping 172.16.1.239","title":"Designate Deployment Manager"},{"location":"deploy/multi-node/#edit-inventory-file","text":"A deployment is performed using an Ansible playbooks, and target systems are defined in an Ansible inventory file . In ROCK this file is located at /etc/rocknsm/hosts.ini . We need to add additional ROCK sensors to the following sections of the inventory file: [rock] [web] [zookeeper] [es_masters] [es_data] [es_ingest]","title":"Edit Inventory File"},{"location":"deploy/multi-node/#rock","text":"This group defines what nodes will be running ROCK services (Zeek, Suricata, Stenographer, Kafka, Zookeeper) example: [rock] rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239","title":"[rock]"},{"location":"deploy/multi-node/#web","text":"This group defines what node will be running web services (Kibana, Lighttpd, and Docket). example: [web] rock03.rock.lan ansible_host=172.16.1.239","title":"[web]"},{"location":"deploy/multi-node/#zookeeper","text":"This group defines what node(s) will be running zookeeper (Kafka cluster manager). example: [zookeeper] rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local","title":"[zookeeper]"},{"location":"deploy/multi-node/#es_masters","text":"This group defines what node(s) will be running Elasticsearch, acting as master nodes. example: [es_masters] rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239","title":"[es_masters]"},{"location":"deploy/multi-node/#es_data","text":"This group defines what node(s) will be running Elasticsearch, acting as data nodes. example: [es_data] rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239","title":"[es_data]"},{"location":"deploy/multi-node/#es_ingest","text":"This group defines what node(s) will be running Elasticsearch, acting as ingest nodes. For ROCK deployments, generally, everything that is data node eligible is also ingest node eligible. example: [es_ingest] rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239","title":"[es_ingest]"},{"location":"deploy/multi-node/#example-inventory","text":"Above, we broke out every section. Let's take a look at a cumulative example of all the above sections in host.ini file for this demo: Simple example inventory: [rock] rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239 [web] rock03.rock.lan ansible_host=172.16.1.239 [lighttpd:children] web [sensors:children] rock [bro:children] sensors [fsf:children] sensors [kafka:children] sensors [stenographer:children] sensors [suricata:children] sensors [filebeat:children] fsf suricata [zookeeper] rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local [elasticsearch:children] es_masters es_data es_ingest [es_masters] # This group should only ever contain exactly 1 or 3 nodes! #simplerockbuild.simplerock.lan ansible_host=127.0.0.1 ansible_connection=local # Multi-node example # #elasticsearch0[1:3].simplerock.lan rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239 [es_data] #simplerockbuild.simplerock.lan ansible_host=127.0.0.1 ansible_connection=local # Multi-node example # #elasticsearch0[1:4].simplerock.lan rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239 [es_ingest] #simplerockbuild.simplerock.lan ansible_host=127.0.0.1 ansible_connection=local # Multi-node example # #elasticsearch0[1:4].simplerock.lan rock01.rock.lan ansible_host=127.0.0.1 ansible_connection=local rock02.rock.lan ansible_host=172.16.1.237 rock03.rock.lan ansible_host=172.16.1.239 [elasticsearch:vars] # Disable all node roles by default node_master=false node_data=false node_ingest=false [es_masters:vars] node_master=true [es_data:vars] node_data=true [es_ingest:vars] node_ingest=true [docket:children] web [kibana:children] web [logstash:children] sensors If you have a bit of a more complex use-case, here's a more extensive example that has: 3 Elasticsearch master nodes 4 Elasticsearch data nodes 1 Elasticsearch coordinating node 1 Logstash node 2 ROCK sensors We also added a few more sections, [Logstash] and [Elasticsearch] . We did this because we're breaking Logstash out into it's own server, as well as adding a coordinating node that will just be part of Elasticsearch, but not a data, ingest, or master node. # Define all of our Ansible_hosts up front es-master-0.rock.lan ansible_host=192.168.1.5 es-master-1.rock.lan ansible_host=192.168.1.6 es-master-2.rock.lan ansible_host=192.168.1.7 es-data-0.rock.lan ansible_host=192.168.1.8 es-data-1.rock.lan ansible_host=192.168.1.9 es-data-2.rock.lan ansible_host=192.168.1.10 es-data-3.rock.lan ansible_host=192.168.1.11 es-coord-0.rock.lan ansible_host=192.168.1.12 ls-pipeline-0.rock.lan ansible_host=192.168.1.13 rock-0.rock.lan ansible_host=192.168.1.14 rock-1.rock.lan ansible_host=192.168.1.15 [rock] rock-0.rock.lan rock-1.rock.lan [web] es-coord-0.rock.lan [lighttpd:children] web [sensors:children] rock [zeek:children] sensors [fsf:children] sensors [kafka:children] sensors [stenographer:children] sensors [suricata:children] sensors [filebeat:children] fsf suricata [zookeeper] rock-0.rock.lan [elasticsearch:children] es_masters es_data es_ingest [elasticsearch] es-coord-0.rock.lan [es_masters] # This group should only ever contain exactly 1 or 3 nodes! # Multi-node example # #elasticsearch0[1:3].simplerock.lan es-master-[0:2].rock.lan [es_data] # Multi-node example # #elasticsearch0[1:4].simplerock.lan es-data-[0:3].rock.lan [es_ingest] # Multi-node example # #elasticsearch0[1:4].simplerock.lan es-data-[0:3].rock.lan [elasticsearch:vars] # Disable all node roles by default node_master=false node_data=false node_ingest=false [es_masters:vars] node_master=true [es_data:vars] node_data=true [es_ingest:vars] node_ingest=true [docket:children] web [kibana:children] web [logstash] ls-pipeline-0.rock.lan","title":"Example Inventory"},{"location":"deploy/multi-node/#ssh-config","text":"After the inventory file entries are finalized, it's time to configure ssh in order for Ansible to communicate to all nodes during deployment. The ssh-config command will perform the following on all other nodes in the inventory: add an authorized keys entry add the user created at install to the sudoers file To configure ssh run: sudo rock ssh-config","title":"SSH Config"},{"location":"deploy/multi-node/#update-rock-configuration-file","text":"There are a few last steps, both are in /etc/rocknsm/config.yml If you are doing an offline installation, you need to change rock_online_install: True to rock_online_install: False . If you are going to have 2 different sensor components, you need to manually define your Zookeeper host by adding kafka_zookeeper_host: Zookeeper IP to the configuration file. It isn't necessary where it is placed.","title":"Update ROCK Configuration File"},{"location":"deploy/multi-node/#multi-node-deployment","text":"Now, we're finally ready to deploy! sudo rock deploy","title":"Multi-node Deployment"},{"location":"deploy/multi-node/#success","text":"Once the deployment is completed with the components you chose, you'll be congratulated with a success banner. Congratulations! We strongly recommend giving all of the services a fresh stop/start with sudo rockctl restart .","title":"Success"},{"location":"deploy/single-node/","text":"Single Node Deployment \u00b6 Let's get started and deploy a single ROCK sensor. This is the most straight forward (and most common) way to deploy. The TUI is an interactive user experience that improves the configuration process, rather than manually editing a .yml file. But to be clear, the end goal of all the selections made in the TUI will be writing out to /etc/rocknsm/config.yml . Requirements \u00b6 The following steps assume that you have done a fresh installation of ROCK using the latest ISO . Deploy using the TUI \u00b6 Let's walk through this menu line by line, starting with the first line that's already highlighted and ready to hit : Start things up by running: $ sudo rock setup Select Interfaces \u00b6 This is where we define what interfaces are used for what purposes. In this example there are 2 interfaces to work with: ens33 - currently connected, will use for remote management ens34 - not currently connected, intended for monitor interface Choose the interfaces you want to for your usecase, and the settings will be reviewed before exiting: Management IP \u00b6 Let's choose how the management IP address is defined. We can let DHCP decide, or set a static address: For this example we'll use DHCP. Set Hostname \u00b6 The next set allows you to set the local hostname. For this example we'll call this box rock01.rock.lan . Offline/Online \u00b6 There are 2 options when selecting the source of installation packages: Yes = Online - connects to upstream repositories for packages No = Offline - uses local packages ( located in /srv ) Choose Components \u00b6 Next up is selecting what components are installed during the deploy. These choices are given to provide flexibility in choosing only the services you want (or have the capability) to run. In the screen below you can see we've selected all the things. Choose Enabled Services \u00b6 After selecting what components are installed, we get a similar interface for selecting which of those services are enabled (configured to start automatically on initial boot). Display Config \u00b6 This next selection gives the opportunity to pause and verify the configuration choices made and review if any changes need to be made. Write Config \u00b6 After reviewing the config, we can write out the changes to disk. Run Installer \u00b6 Finally, we can kick off our deployment and ROCK things thing. Success \u00b6 Once the deployment is completed with the components you chose, you'll be congratulated with a success banner. Congratulations! We strongly recommend giving all of the services a fresh stop/start with sudo rockctl restart .","title":"Single Node"},{"location":"deploy/single-node/#single-node-deployment","text":"Let's get started and deploy a single ROCK sensor. This is the most straight forward (and most common) way to deploy. The TUI is an interactive user experience that improves the configuration process, rather than manually editing a .yml file. But to be clear, the end goal of all the selections made in the TUI will be writing out to /etc/rocknsm/config.yml .","title":"Single Node Deployment"},{"location":"deploy/single-node/#requirements","text":"The following steps assume that you have done a fresh installation of ROCK using the latest ISO .","title":"Requirements"},{"location":"deploy/single-node/#deploy-using-the-tui","text":"Let's walk through this menu line by line, starting with the first line that's already highlighted and ready to hit : Start things up by running: $ sudo rock setup","title":"Deploy using the TUI"},{"location":"deploy/single-node/#select-interfaces","text":"This is where we define what interfaces are used for what purposes. In this example there are 2 interfaces to work with: ens33 - currently connected, will use for remote management ens34 - not currently connected, intended for monitor interface Choose the interfaces you want to for your usecase, and the settings will be reviewed before exiting:","title":"Select Interfaces"},{"location":"deploy/single-node/#management-ip","text":"Let's choose how the management IP address is defined. We can let DHCP decide, or set a static address: For this example we'll use DHCP.","title":"Management IP"},{"location":"deploy/single-node/#set-hostname","text":"The next set allows you to set the local hostname. For this example we'll call this box rock01.rock.lan .","title":"Set Hostname"},{"location":"deploy/single-node/#offlineonline","text":"There are 2 options when selecting the source of installation packages: Yes = Online - connects to upstream repositories for packages No = Offline - uses local packages ( located in /srv )","title":"Offline/Online"},{"location":"deploy/single-node/#choose-components","text":"Next up is selecting what components are installed during the deploy. These choices are given to provide flexibility in choosing only the services you want (or have the capability) to run. In the screen below you can see we've selected all the things.","title":"Choose Components"},{"location":"deploy/single-node/#choose-enabled-services","text":"After selecting what components are installed, we get a similar interface for selecting which of those services are enabled (configured to start automatically on initial boot).","title":"Choose Enabled Services"},{"location":"deploy/single-node/#display-config","text":"This next selection gives the opportunity to pause and verify the configuration choices made and review if any changes need to be made.","title":"Display Config"},{"location":"deploy/single-node/#write-config","text":"After reviewing the config, we can write out the changes to disk.","title":"Write Config"},{"location":"deploy/single-node/#run-installer","text":"Finally, we can kick off our deployment and ROCK things thing.","title":"Run Installer"},{"location":"deploy/single-node/#success","text":"Once the deployment is completed with the components you chose, you'll be congratulated with a success banner. Congratulations! We strongly recommend giving all of the services a fresh stop/start with sudo rockctl restart .","title":"Success"},{"location":"deploy/terminology/","text":"Terminology \u00b6 Deployment Types \u00b6 Before getting too deep into the bits let's make sure we're using the same terminology for things. There are 2 main ways to deploy RockNSM: Single Node Multi Node Single Node \u00b6 A \"Single Node Deployment\" is used to deploy one standalone ROCK sensor. This is the simplest scenario and the most common use case. Multi-Node \u00b6 A \"Muti-Node Deployment\" is used to interconnect a collection of ROCK sensors that have already been installed and can talk to each other on the same network. This requires a bit more work up front to deploy, but is allows for a lot of scaling and flexibility to share workload across multiple boxes.","title":"Terminology"},{"location":"deploy/terminology/#terminology","text":"","title":"Terminology"},{"location":"deploy/terminology/#deployment-types","text":"Before getting too deep into the bits let's make sure we're using the same terminology for things. There are 2 main ways to deploy RockNSM: Single Node Multi Node","title":"Deployment Types"},{"location":"deploy/terminology/#single-node","text":"A \"Single Node Deployment\" is used to deploy one standalone ROCK sensor. This is the simplest scenario and the most common use case.","title":"Single Node"},{"location":"deploy/terminology/#multi-node","text":"A \"Muti-Node Deployment\" is used to interconnect a collection of ROCK sensors that have already been installed and can talk to each other on the same network. This requires a bit more work up front to deploy, but is allows for a lot of scaling and flexibility to share workload across multiple boxes.","title":"Multi-Node"},{"location":"install/install/","text":"Installation Guide \u00b6 Let's just hold on for one hot minute before installing, just in case you like to skip around. It's dangerous to deploy sensors alone... take these minumum requirements : 8GB RAM 4 Physical Cores 256GB disk 2 NICs active connection on mgmt port Network Connection \u00b6 This is a critical setup point: Before booting to the ISO, connect the network interface that you intend to use to remotely manage ROCK. Why? During install, ROCK will see the network interface with an ip address and default gateway and designate it as the management port. So plug into that interface and boot to your USB drive. Install Types \u00b6 ROCK works with both legacy BIOS and UEFI booting. Once booted from the USB, you are presented with 2 primary installation paths: Automated Custom Automated \u00b6 The \"Automated\" option is intended to serve as a starting point that allows you to get into things. It uses the Centos Anaconda installer to make some of the harder decisions for users by skipping over many options to get you up and running. It makes a best guess at how to use resources -- most notably how to manage available disks. Bottom line: think of this as a product quickstart mode, perfect for installing on a VM or other temporary hardware. It is not for production sensor deployment. For the rest of this install guide we'll work through the more detail oriented \"Custom Install of ROCK\" option. Custom \u00b6 The \"Custom\" allows for more customization of a ROCK installation. This is especially helpful when you're working with multiple disks or even a large amount of storage on a single disk. The Custom option is recommended for production environments in order to get more granular in choosing how disk space is allocated. Disk Allocation \u00b6 Configuring disk and storage is a deep topic on it's own, but let's talke about a few examples to get started: Stenographer \u00b6 A common gotcha occurs when you want full packet capture (via Stenographer ), but it isn't given a separate partition. Stenographer is great at managing it's own disk space (starts to overwrite oldest data at 90% capacity), but that doesn't cut it when it's sharing the same mount point as Bro, Suricata , and other tools that generate data in ROCK. Best practice would be to create a /data/stenographer partition in order to prevent limited operations. For example, Elasticsearch will (rightfully) lock indexes up to a read-only state in order to keep things from crashing hard. System Logs \u00b6 Another useful partition to create is /var/log to separate system log files from the rest of the system. Example Table \u00b6 Below is a good starting point when partitioning MOUNT POINT USAGE SIZE SYSTEM SYSTEM SYSTEM / root filesystem 15 GiB /boot legacy boot files 512 MiB /boot/efi uefi boot files 512 MiB swap memory shortage ~8 GiB+ DATA DATA DATA /var/log system log files ~15 GiB /home user home dirs ~20 GiB /data data partition ~ GiB /data/stenographer steno partition ~ GiB For more information to assist with the partitioning process, you can see the RHEL guide . Also, it may be a bit more self explanatory for you if you click \u201cautomatic partitions\u201d then modify accordingly. Date & Time \u00b6 UTC is generally preferred for logging data as the timestamps from anywhere in the world will have a proper order without calculating offsets and daylight savings. That said, Kibana will present the Bro logs according to your timezone (as set in the browser). The bro logs themselves (i.e. in /data/bro/logs/) log in epoch time and will be written in UTC regardless of the system timezone. Bro includes a utility for parsing these on the command line called bro-cut . It can be used to print human-readable timestamps in either the local sensor timezone or UTC. You can also give it a custom format string to specify what you'd like displayed. Network & Hostname \u00b6 Before beginning the install process it's best to connect the interface you've selected to be the management interface . Here's the order of events: ROCK will initially look for an interface with a default gateway and treat that interface as the MGMT INTERFACE All remaining interfaces will be treated as MONITOR INTERFACES Ensure that the interface you intend to use for MGMT has been turned on and has an ip address Set the hostname of the sensor in the bottom left corner this hostname will populate the Ansible inventory file in /etc/rocknsm/hosts.ini Do not use a hostname that contains an underscore, for example rock_2-5 . This will cause deployment failures! User Creation \u00b6 ROCK is configured with the root user disabled. We recommend that you leave it that way. Once you've kicked off the install, click User Creation at the next screen (shown above) and complete the required fields to set up a non-root admin user. If this step is not completed during install, do not fear. you will be prompted to create this account after first login. Wrapping Up \u00b6 Once the install is complete you will be able to click Finish Installation and then reboot. You can then accept the license agreement: c (ontinue) + ENTER The sshd services is enabled at startup, so if you intend to complete the next steps remotely, note the management ip address now by running ip a . Your newly minted sensor is ready to be configured for your environment.","title":"Installation"},{"location":"install/install/#installation-guide","text":"Let's just hold on for one hot minute before installing, just in case you like to skip around. It's dangerous to deploy sensors alone... take these minumum requirements : 8GB RAM 4 Physical Cores 256GB disk 2 NICs active connection on mgmt port","title":"Installation Guide"},{"location":"install/install/#network-connection","text":"This is a critical setup point: Before booting to the ISO, connect the network interface that you intend to use to remotely manage ROCK. Why? During install, ROCK will see the network interface with an ip address and default gateway and designate it as the management port. So plug into that interface and boot to your USB drive.","title":"Network Connection"},{"location":"install/install/#install-types","text":"ROCK works with both legacy BIOS and UEFI booting. Once booted from the USB, you are presented with 2 primary installation paths: Automated Custom","title":"Install Types"},{"location":"install/install/#automated","text":"The \"Automated\" option is intended to serve as a starting point that allows you to get into things. It uses the Centos Anaconda installer to make some of the harder decisions for users by skipping over many options to get you up and running. It makes a best guess at how to use resources -- most notably how to manage available disks. Bottom line: think of this as a product quickstart mode, perfect for installing on a VM or other temporary hardware. It is not for production sensor deployment. For the rest of this install guide we'll work through the more detail oriented \"Custom Install of ROCK\" option.","title":"Automated"},{"location":"install/install/#custom","text":"The \"Custom\" allows for more customization of a ROCK installation. This is especially helpful when you're working with multiple disks or even a large amount of storage on a single disk. The Custom option is recommended for production environments in order to get more granular in choosing how disk space is allocated.","title":"Custom"},{"location":"install/install/#disk-allocation","text":"Configuring disk and storage is a deep topic on it's own, but let's talke about a few examples to get started:","title":"Disk Allocation"},{"location":"install/install/#stenographer","text":"A common gotcha occurs when you want full packet capture (via Stenographer ), but it isn't given a separate partition. Stenographer is great at managing it's own disk space (starts to overwrite oldest data at 90% capacity), but that doesn't cut it when it's sharing the same mount point as Bro, Suricata , and other tools that generate data in ROCK. Best practice would be to create a /data/stenographer partition in order to prevent limited operations. For example, Elasticsearch will (rightfully) lock indexes up to a read-only state in order to keep things from crashing hard.","title":"Stenographer"},{"location":"install/install/#system-logs","text":"Another useful partition to create is /var/log to separate system log files from the rest of the system.","title":"System Logs"},{"location":"install/install/#example-table","text":"Below is a good starting point when partitioning MOUNT POINT USAGE SIZE SYSTEM SYSTEM SYSTEM / root filesystem 15 GiB /boot legacy boot files 512 MiB /boot/efi uefi boot files 512 MiB swap memory shortage ~8 GiB+ DATA DATA DATA /var/log system log files ~15 GiB /home user home dirs ~20 GiB /data data partition ~ GiB /data/stenographer steno partition ~ GiB For more information to assist with the partitioning process, you can see the RHEL guide . Also, it may be a bit more self explanatory for you if you click \u201cautomatic partitions\u201d then modify accordingly.","title":"Example Table"},{"location":"install/install/#date-time","text":"UTC is generally preferred for logging data as the timestamps from anywhere in the world will have a proper order without calculating offsets and daylight savings. That said, Kibana will present the Bro logs according to your timezone (as set in the browser). The bro logs themselves (i.e. in /data/bro/logs/) log in epoch time and will be written in UTC regardless of the system timezone. Bro includes a utility for parsing these on the command line called bro-cut . It can be used to print human-readable timestamps in either the local sensor timezone or UTC. You can also give it a custom format string to specify what you'd like displayed.","title":"Date &amp; Time"},{"location":"install/install/#network-hostname","text":"Before beginning the install process it's best to connect the interface you've selected to be the management interface . Here's the order of events: ROCK will initially look for an interface with a default gateway and treat that interface as the MGMT INTERFACE All remaining interfaces will be treated as MONITOR INTERFACES Ensure that the interface you intend to use for MGMT has been turned on and has an ip address Set the hostname of the sensor in the bottom left corner this hostname will populate the Ansible inventory file in /etc/rocknsm/hosts.ini Do not use a hostname that contains an underscore, for example rock_2-5 . This will cause deployment failures!","title":"Network &amp; Hostname"},{"location":"install/install/#user-creation","text":"ROCK is configured with the root user disabled. We recommend that you leave it that way. Once you've kicked off the install, click User Creation at the next screen (shown above) and complete the required fields to set up a non-root admin user. If this step is not completed during install, do not fear. you will be prompted to create this account after first login.","title":"User Creation"},{"location":"install/install/#wrapping-up","text":"Once the install is complete you will be able to click Finish Installation and then reboot. You can then accept the license agreement: c (ontinue) + ENTER The sshd services is enabled at startup, so if you intend to complete the next steps remotely, note the management ip address now by running ip a . Your newly minted sensor is ready to be configured for your environment.","title":"Wrapping Up"},{"location":"install/media/","text":"Install Media \u00b6 If there\u2019s one thing that should be carried away from the installation section, it's this: RockNSM has been designed to be used as a security distribution, not a package or a suite of tools. It\u2019s built from the ground up and the ONLY SUPPORTED INSTALL IS THE OFFICIAL ISO. Yes, one can clone the project and run the Ansible on some bespoke CentOS build, and you may have great success... but you've voided the warranty . Providing a clean product that makes supporting submitted issues is important to us. The ISO addresses most use cases. Download \u00b6 The lastest ROCK build is available at download.rocknsm.io . Applying the ISO \u00b6 Now it's time to create a bootable USB drive with the fresh ROCK build. Let's look at few options. Linux \u00b6 CLI \u00b6 If you live in the terminal, use dd to apply the image. These instructions are for using a RHEL based system. If you're in a different environment, google is your friend. CAUTION when using these commands by ENSURING you're writing to the correct disk / partition! once you've inserted a USB get the drive ID: lsblk unmount the target drive so you can write to it: umount /dev/disk# write the image to drive: sudo dd bs=8M if=path/to/rockiso of=/dev/disk# GUI \u00b6 If you don't want to apply the image in the terminal, there are plenty of great tools to do this with a graphical interface: Etcher - our go-to choice (cross-platform) SD Card Formatter - works well YUMI - create multibooting disk Note while we do not want to dictate what tool you use, we've received reports when using Rufus, a popular Windows based tool, to make bootable media. We reported this on Feb 26 https://twitter.com/rocknsm/status/1100517122021748737. The above GUI tools have been proven to work. macOS \u00b6 CLI \u00b6 For the terminal, we'll once again use dd , but with a few differences from the linux instructions above. CAUTION when using these commands by ENSURING you're writing to the correct disk / partition! once you've inserted a USB get the drive ID: diskutil list unmount the target drive so you can write to it: diskutil unmount /dev/disk# write the image to drive: sudo dd bs=8m if=path/to/rockiso of=/dev/disk# GUI \u00b6 Etcher - our defacto tool Windows \u00b6 When applying ISO on a Windows box our experience is with graphical applications entirely. Here's a list of what works well: Etcher - our defacto tool Win32 Disk Imager SD Card Formatter Rufus - (we recently encountered an issue with Rufus and v2.3 -- 2/26/19)","title":"Media"},{"location":"install/media/#install-media","text":"If there\u2019s one thing that should be carried away from the installation section, it's this: RockNSM has been designed to be used as a security distribution, not a package or a suite of tools. It\u2019s built from the ground up and the ONLY SUPPORTED INSTALL IS THE OFFICIAL ISO. Yes, one can clone the project and run the Ansible on some bespoke CentOS build, and you may have great success... but you've voided the warranty . Providing a clean product that makes supporting submitted issues is important to us. The ISO addresses most use cases.","title":"Install Media"},{"location":"install/media/#download","text":"The lastest ROCK build is available at download.rocknsm.io .","title":"Download"},{"location":"install/media/#applying-the-iso","text":"Now it's time to create a bootable USB drive with the fresh ROCK build. Let's look at few options.","title":"Applying the ISO"},{"location":"install/media/#linux","text":"","title":"Linux"},{"location":"install/media/#cli","text":"If you live in the terminal, use dd to apply the image. These instructions are for using a RHEL based system. If you're in a different environment, google is your friend. CAUTION when using these commands by ENSURING you're writing to the correct disk / partition! once you've inserted a USB get the drive ID: lsblk unmount the target drive so you can write to it: umount /dev/disk# write the image to drive: sudo dd bs=8M if=path/to/rockiso of=/dev/disk#","title":"CLI"},{"location":"install/media/#gui","text":"If you don't want to apply the image in the terminal, there are plenty of great tools to do this with a graphical interface: Etcher - our go-to choice (cross-platform) SD Card Formatter - works well YUMI - create multibooting disk Note while we do not want to dictate what tool you use, we've received reports when using Rufus, a popular Windows based tool, to make bootable media. We reported this on Feb 26 https://twitter.com/rocknsm/status/1100517122021748737. The above GUI tools have been proven to work.","title":"GUI"},{"location":"install/media/#macos","text":"","title":"macOS"},{"location":"install/media/#cli_1","text":"For the terminal, we'll once again use dd , but with a few differences from the linux instructions above. CAUTION when using these commands by ENSURING you're writing to the correct disk / partition! once you've inserted a USB get the drive ID: diskutil list unmount the target drive so you can write to it: diskutil unmount /dev/disk# write the image to drive: sudo dd bs=8m if=path/to/rockiso of=/dev/disk#","title":"CLI"},{"location":"install/media/#gui_1","text":"Etcher - our defacto tool","title":"GUI"},{"location":"install/media/#windows","text":"When applying ISO on a Windows box our experience is with graphical applications entirely. Here's a list of what works well: Etcher - our defacto tool Win32 Disk Imager SD Card Formatter Rufus - (we recently encountered an issue with Rufus and v2.3 -- 2/26/19)","title":"Windows"},{"location":"install/requirements/","text":"Requirements \u00b6 Installation of ROCK can be broken down into three main steps: Install Configure Deploy Before that, let's cover what you're going to need before starting. Sensor Hardware \u00b6 The analysis of live network data is a resource intensive task, so the higher the IOPS the better. Here's the bottom line: If you throw hardware at ROCK it will use it, and use it well. Minimum Specs \u00b6 RESOURCE RECOMMENDATION CPU 4+ physical cores Memory 8GB RAM minimum, the more the better :) Storage 256GB , with 200+ of that dedicated to /data , SSD preferred Network 2 gigabit interfaces , one for management and one for collection Install Media \u00b6 ROCK install image - download .iso here 8GB+ capacity USB drive - to apply install image BIOS settings to allow booting from mounted USB drive Network Connection \u00b6 ROCK is first and foremost a passive network sensor and is designed with the assumption that there may not be a network connection available during install. There's some built-in flexibility with deploying ROCK, and this will be clarified more in then next sections. NOTE: Check out the ROCK@home Video Series that goes into detail on many things about deploying ROCK to include hardware choices for both sensor and network equipment.","title":"Requirements"},{"location":"install/requirements/#requirements","text":"Installation of ROCK can be broken down into three main steps: Install Configure Deploy Before that, let's cover what you're going to need before starting.","title":"Requirements"},{"location":"install/requirements/#sensor-hardware","text":"The analysis of live network data is a resource intensive task, so the higher the IOPS the better. Here's the bottom line: If you throw hardware at ROCK it will use it, and use it well.","title":"Sensor Hardware"},{"location":"install/requirements/#minimum-specs","text":"RESOURCE RECOMMENDATION CPU 4+ physical cores Memory 8GB RAM minimum, the more the better :) Storage 256GB , with 200+ of that dedicated to /data , SSD preferred Network 2 gigabit interfaces , one for management and one for collection","title":"Minimum Specs"},{"location":"install/requirements/#install-media","text":"ROCK install image - download .iso here 8GB+ capacity USB drive - to apply install image BIOS settings to allow booting from mounted USB drive","title":"Install Media"},{"location":"install/requirements/#network-connection","text":"ROCK is first and foremost a passive network sensor and is designed with the assumption that there may not be a network connection available during install. There's some built-in flexibility with deploying ROCK, and this will be clarified more in then next sections. NOTE: Check out the ROCK@home Video Series that goes into detail on many things about deploying ROCK to include hardware choices for both sensor and network equipment.","title":"Network Connection"},{"location":"install/vm_installation/","text":"VM Build Guide \u00b6 The following walkthrough is based on VMware Fusion, but serves well as a general template to follow. The more resources you give ROCK, the happier it'll be. New VM \u00b6 in the top left corner click \" Add > New... then Custom Machine \" select the \" Linux > RedHat Enterprise 64 template \" create new virtual disk name your VM, save Lets customize some settings, change based on your hardware. Processors & Memory Processors - 4 cores Memory - 8192MB (8GB) Hard Disk increase the disk to 20GB customize settings save as name Network Adapter By default the vm is created with one interface - this will be for management. lets add a second (listening) interface: add device (top right), net adapter, add, \u201cprivate to my mac\u201d Boot Device click CD/DVD (IDE) check the \"Connect CD/DVD Drive\" box expand advanced options and browse to the latest ROCK iso Run Auto Installer Once the above changes are made, it's time to install the OS. Lets run the \"Auto Install\", but before we do, there are some customization that can be done for VMs: click the \" Start Up \" button while holding the esc key hit tab for full config options add the following values, speparated by spaces: biosdevname=0 net.ifnames=0 - this will ensure you get interface names like eth0 . If you have physical hardware, I highly recommend that you do not use this function vga=773 - improves video resolution issues ENTER , and ROCK install script will install create admin user acct REBOOT when install process is complete TIP: The root account is locked by default and the user account you created has sudo access. Updating \u00b6 NOTE: VMware Fusion will allow local ssh out of the box. If you're using Virtualbox you'll need to set up local ssh port forwarding . Log in with the admin credentials used during the install process, and lets get this box current: sudo yum update -y && reboot","title":"VM Build Guide"},{"location":"install/vm_installation/#vm-build-guide","text":"The following walkthrough is based on VMware Fusion, but serves well as a general template to follow. The more resources you give ROCK, the happier it'll be.","title":"VM Build Guide"},{"location":"install/vm_installation/#new-vm","text":"in the top left corner click \" Add > New... then Custom Machine \" select the \" Linux > RedHat Enterprise 64 template \" create new virtual disk name your VM, save Lets customize some settings, change based on your hardware. Processors & Memory Processors - 4 cores Memory - 8192MB (8GB) Hard Disk increase the disk to 20GB customize settings save as name Network Adapter By default the vm is created with one interface - this will be for management. lets add a second (listening) interface: add device (top right), net adapter, add, \u201cprivate to my mac\u201d Boot Device click CD/DVD (IDE) check the \"Connect CD/DVD Drive\" box expand advanced options and browse to the latest ROCK iso Run Auto Installer Once the above changes are made, it's time to install the OS. Lets run the \"Auto Install\", but before we do, there are some customization that can be done for VMs: click the \" Start Up \" button while holding the esc key hit tab for full config options add the following values, speparated by spaces: biosdevname=0 net.ifnames=0 - this will ensure you get interface names like eth0 . If you have physical hardware, I highly recommend that you do not use this function vga=773 - improves video resolution issues ENTER , and ROCK install script will install create admin user acct REBOOT when install process is complete TIP: The root account is locked by default and the user account you created has sudo access.","title":"New VM"},{"location":"install/vm_installation/#updating","text":"NOTE: VMware Fusion will allow local ssh out of the box. If you're using Virtualbox you'll need to set up local ssh port forwarding . Log in with the admin credentials used during the install process, and lets get this box current: sudo yum update -y && reboot","title":"Updating"},{"location":"reference/changelog/","text":"Changelog \u00b6 2.5 -- 2019-08-21 \u00b6 New: ROCK has move to the ECS standard New: Out of the box support for XFS Disk Quotas New: Updated ROCK Dashboards Fix: Various visualization issues in ROCK dashboard Fix: (x509) Certificate issues resolved Update: Elastic Stack components to version 7.6 Update: Zeek to version 3 Update: Zeek to version 5 2.4 -- 2019-04-02 \u00b6 New: Text User Interface (TUI) for initial host setup New: ROCK manager utility New: Automated Testing Infrastructure Fixes: 95 closed issues Upgrade: Elastic 6.6 -> 6.7.1 Upgrade: Suricata 4.1.1 -> 4.1.3 Upgrade: Zookeeper 3.4.11 -> 3.4.13 2.3 -- 2019-02-25 \u00b6 New: Add ability to do multi-host deployment of sensor + data tiers (#339) New: Integrate Docket into Kibana by default New: Improvements and additional Kibana dashboards Fixes: issue with Bro failing when monitor interface is down (#343) Fixes: issue with services starting that shouldn\u2019t (#346) Fixes: race condition on loading dashboards into Kibana (#356) Fixes: configuration for Docket allowing serving from non-root URI (#361) Change: bro log retention value to one week rather than forever (#345) Change: Greatly improve documentation (#338) Change: Reorganize README (#308) Change: Move ECS to rock-dashboards repo (#305) Change: Move RockNSM install paths to filesystem hierarchy standard locations (#344) 2.2 -- 2018-10-26 \u00b6 Feature: rockctl command to quickly check or change services Feature: Docket, a REST API and web UI to query multiple stenographer instances, now using TCP port 443 Optimization: Kibana is now running on TCP port 443 Feature: Added Suricata-Update to manage Suricata signatures Feature: GPG signing of packages and repo metadata Feature: Added functional tests using testinfra Feature: Initial support of Elastic Common Schema Feature: Elastic new Features Canvas Elastic Maps Service Feature: Include full Elasticstack (with permission) including features formerly known as X-Pack: Graph Machine Learning Reporting Security Monitoring Alerting Elasticsearch SQL Optimization: Elastic dashboards, mappings, and Logstash config moved to module-like construct Upgrade: CentOS is updated to 7.5 (1804) Upgrade: Elastic Stack is updated to 6.4.2 Upgrade: Suricata is updated to 4.0.5 Upgrade: Bro is updated to 2.5.4 2.1 -- 2018-08-23 \u00b6","title":"Changelog"},{"location":"reference/changelog/#changelog","text":"","title":"Changelog"},{"location":"reference/changelog/#25-2019-08-21","text":"New: ROCK has move to the ECS standard New: Out of the box support for XFS Disk Quotas New: Updated ROCK Dashboards Fix: Various visualization issues in ROCK dashboard Fix: (x509) Certificate issues resolved Update: Elastic Stack components to version 7.6 Update: Zeek to version 3 Update: Zeek to version 5","title":"2.5 -- 2019-08-21"},{"location":"reference/changelog/#24-2019-04-02","text":"New: Text User Interface (TUI) for initial host setup New: ROCK manager utility New: Automated Testing Infrastructure Fixes: 95 closed issues Upgrade: Elastic 6.6 -> 6.7.1 Upgrade: Suricata 4.1.1 -> 4.1.3 Upgrade: Zookeeper 3.4.11 -> 3.4.13","title":"2.4 -- 2019-04-02"},{"location":"reference/changelog/#23-2019-02-25","text":"New: Add ability to do multi-host deployment of sensor + data tiers (#339) New: Integrate Docket into Kibana by default New: Improvements and additional Kibana dashboards Fixes: issue with Bro failing when monitor interface is down (#343) Fixes: issue with services starting that shouldn\u2019t (#346) Fixes: race condition on loading dashboards into Kibana (#356) Fixes: configuration for Docket allowing serving from non-root URI (#361) Change: bro log retention value to one week rather than forever (#345) Change: Greatly improve documentation (#338) Change: Reorganize README (#308) Change: Move ECS to rock-dashboards repo (#305) Change: Move RockNSM install paths to filesystem hierarchy standard locations (#344)","title":"2.3 -- 2019-02-25"},{"location":"reference/changelog/#22-2018-10-26","text":"Feature: rockctl command to quickly check or change services Feature: Docket, a REST API and web UI to query multiple stenographer instances, now using TCP port 443 Optimization: Kibana is now running on TCP port 443 Feature: Added Suricata-Update to manage Suricata signatures Feature: GPG signing of packages and repo metadata Feature: Added functional tests using testinfra Feature: Initial support of Elastic Common Schema Feature: Elastic new Features Canvas Elastic Maps Service Feature: Include full Elasticstack (with permission) including features formerly known as X-Pack: Graph Machine Learning Reporting Security Monitoring Alerting Elasticsearch SQL Optimization: Elastic dashboards, mappings, and Logstash config moved to module-like construct Upgrade: CentOS is updated to 7.5 (1804) Upgrade: Elastic Stack is updated to 6.4.2 Upgrade: Suricata is updated to 4.0.5 Upgrade: Bro is updated to 2.5.4","title":"2.2 -- 2018-10-26"},{"location":"reference/changelog/#21-2018-08-23","text":"","title":"2.1 -- 2018-08-23"},{"location":"reference/contribution/","text":"How to Pitch In \u00b6 ROCK is an open source project and would not be what it is without a community of users and contributors. There are many ways to contribute, so take a look at how: General Support \u00b6 For quick questions and deployment support, please join the RockNSM Community . Github Contribution \u00b6 Issues \u00b6 Before you submit an issue please search the issue tracker, as there may already be an issue for your problem and it's discussion might just solve things. We want to resolve issues as soon as possible, but in order to do so please provide as much detail about the environment and situation as possible. Pull Requests \u00b6 In order to issue a PR, fork the project, make your changes, and add descriptive messages to your commits (this is a mandatory requirement for your PR to be considered). Submit a PR to the main rock repo . If changes or additions are suggested, edit your fork and this will automatically update your PR.","title":"Contribution"},{"location":"reference/contribution/#how-to-pitch-in","text":"ROCK is an open source project and would not be what it is without a community of users and contributors. There are many ways to contribute, so take a look at how:","title":"How to Pitch In"},{"location":"reference/contribution/#general-support","text":"For quick questions and deployment support, please join the RockNSM Community .","title":"General Support"},{"location":"reference/contribution/#github-contribution","text":"","title":"Github Contribution"},{"location":"reference/contribution/#issues","text":"Before you submit an issue please search the issue tracker, as there may already be an issue for your problem and it's discussion might just solve things. We want to resolve issues as soon as possible, but in order to do so please provide as much detail about the environment and situation as possible.","title":"Issues"},{"location":"reference/contribution/#pull-requests","text":"In order to issue a PR, fork the project, make your changes, and add descriptive messages to your commits (this is a mandatory requirement for your PR to be considered). Submit a PR to the main rock repo . If changes or additions are suggested, edit your fork and this will automatically update your PR.","title":"Pull Requests"},{"location":"reference/latest/","text":"Latest \u00b6 Release 2.5 \u00b6 We are pleased to announce that ROCK 2.5 is out! Here's a quick overview of some of the latest additions: NEW - ROCK has move to the ECS standard! legacy pipeline is still available (on ISO install) aliases are in place to assist backwards compatibility NEW - Out of the box support for XFS Disk Quotas puts quota on /data or falls back to / works for both automated and manual installs standalone playbook to setup quotas on installs other than ISO download (reboot req.) the amount of disk given to a service is enabled by weight NEW - Updated ROCK Dashboards available in ISO install incorporating Vega into dashboards FIX - various visualization issues in ROCK dashboard FIX - x509 certificate issues resolved UPDATE - All Elastic Stack components to v7.6 NEW - Updated Zeek to version 3 NEW - Updated Suricata to version 5","title":"Latest Release"},{"location":"reference/latest/#latest","text":"","title":"Latest"},{"location":"reference/latest/#release-25","text":"We are pleased to announce that ROCK 2.5 is out! Here's a quick overview of some of the latest additions: NEW - ROCK has move to the ECS standard! legacy pipeline is still available (on ISO install) aliases are in place to assist backwards compatibility NEW - Out of the box support for XFS Disk Quotas puts quota on /data or falls back to / works for both automated and manual installs standalone playbook to setup quotas on installs other than ISO download (reboot req.) the amount of disk given to a service is enabled by weight NEW - Updated ROCK Dashboards available in ISO install incorporating Vega into dashboards FIX - various visualization issues in ROCK dashboard FIX - x509 certificate issues resolved UPDATE - All Elastic Stack components to v7.6 NEW - Updated Zeek to version 3 NEW - Updated Suricata to version 5","title":"Release 2.5"},{"location":"reference/license/","text":"Liscense \u00b6 Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"reference/license/#liscense","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Liscense"},{"location":"reference/support/","text":"Support \u00b6 Community Support \u00b6 For community support using ROCK, we encourage you to check out the fine members over at the RockNSM Community Site . From there you'll be able to connect with other users of ROCK as well as the developers and maintainers of the project. This is by far the fastest way to connect with others as well as start to troubleshoot issues you may be experiencing. GitHub Issue \u00b6 In the event that you identify an issue with the project, please feel free to log it using the GitHub Issue tracker for the project. Please be sure to include the contents of /etc/rocknsm/rocknsm-buildstamp .","title":"Commercial Support"},{"location":"reference/support/#support","text":"","title":"Support"},{"location":"reference/support/#community-support","text":"For community support using ROCK, we encourage you to check out the fine members over at the RockNSM Community Site . From there you'll be able to connect with other users of ROCK as well as the developers and maintainers of the project. This is by far the fastest way to connect with others as well as start to troubleshoot issues you may be experiencing.","title":"Community Support"},{"location":"reference/support/#github-issue","text":"In the event that you identify an issue with the project, please feel free to log it using the GitHub Issue tracker for the project. Please be sure to include the contents of /etc/rocknsm/rocknsm-buildstamp .","title":"GitHub Issue"},{"location":"reference/tutorials/","text":"Tutorials \u00b6 We've been working hard to create clear and relevant training content: Video Guides \u00b6 ROCK Introduction \u00b6 ROCK Introduction - what ROCK is and how everything works together ROCK@home Series \u00b6 ROCK@home - 3 part series on the lowest barrier to entry: tapping your home network BSidesKC 2018 \u00b6 Threat Hunting with RockNSM - this talk by Bradford Dabbs discusses the benefits of a passive first approach and how RockNSM can be used to facilitate it.","title":"Tutorials and Videos"},{"location":"reference/tutorials/#tutorials","text":"We've been working hard to create clear and relevant training content:","title":"Tutorials"},{"location":"reference/tutorials/#video-guides","text":"","title":"Video Guides"},{"location":"reference/tutorials/#rock-introduction","text":"ROCK Introduction - what ROCK is and how everything works together","title":"ROCK Introduction"},{"location":"reference/tutorials/#rockhome-series","text":"ROCK@home - 3 part series on the lowest barrier to entry: tapping your home network","title":"ROCK@home Series"},{"location":"reference/tutorials/#bsideskc-2018","text":"Threat Hunting with RockNSM - this talk by Bradford Dabbs discusses the benefits of a passive first approach and how RockNSM can be used to facilitate it.","title":"BSidesKC 2018"},{"location":"services/","text":"Summary \u00b6 ROCK uses a collection of open-source applications as described in this \"Services\" section. This portion of the documentation covers the basic administration of each of the major components of RockNSM. Managing All Services \u00b6 If you know Linux operating systems that use systemd, the you'll be familiar with the command systemctl to manage services. ROCK provides two separate scripts that provide similar functionality, depending upon your deployment scenario. $ rockctl - service manage for a single-node instance $ rock - service manage for a multi-node instance rockctl \u00b6 The rockctl command is a Bash script that calls systemd to action all ROCK services. Use this tool when you're connected to a specific sensor and need to take action on the sensor services as a group. rockctl can be used to perform the following operations: $ sudo rockctl status # Report status for all ROCK services on this host stop # Stop all ROCK services on this host start # Start all ROCK services on this host restart # Restart all ROCK services on this host reset-failed # Reset failed status all ROCK services on this host # Useful for services like stenographer that start and exit rock \u00b6 The rock command is a Bash script that uses Ansible to action services across all sensors across all sensors that are part of a multi-node deployment. [ admin@rock ~ ] $ rock Commands: setup Launch TUI to configure this host for deployment tui Alias for setup ssh-config Configure hosts in inventory to use key-based auth ( multinode ) deploy Deploy selected ROCK components deploy-offline Same as deploy --offline ( Default ISO behavior ) deploy-online Same as deploy --online stop Stop all ROCK services start Start all ROCK services restart Restart all ROCK services status Report status for all ROCK services genconfig Generate default configuration based on current system destroy Destroy all ROCK data: indexes, logs, PCAP, i.e. EVERYTHING NOTE: Will not remove any services, just the data Options: --config, -c <config_yaml> Specify full path to configuration overrides --extra, -e <ansible variables> Set additional variables as key = value or YAML/JSON passed to ansible-playbook --help, -h Show this usage information --inventory, -i <inventory_path> Specify path to Ansible inventory file --limit <host> Specify host to run plays --list-hosts Outputs a list of matching hosts ; does not execute anything else --list-tags List all available tags --list-tasks List all tasks that would be executed --offline, -o Deploy ROCK using only local repos ( Default ISO behavior ) --online, -O Deploy ROCK using online repos --playbook, -p <playbook_path> Specify path to Ansible playbook file --skip-tags <tags> Only run plays and tasks whose tags do not match these values --tags, -t <tags> Only run plays and tasks tagged with these values --verbose, -v Increase verbosity of ansible-playbook Managing Single Services \u00b6 When you need to take action on a singular service (a la carte), use systemctl as you would Published URLs and Ports \u00b6 ROCK uses the lighttpd webserver to perform vhost redirects to it's web interfaces. It's configured to listen for (IPV4 only) connections over port 443 for the following: URLs \u00b6 Kibana is accessible at: https://<sensorip>/app/kibana Docket is accessible at: https://<sensorip>/app/docket/ Ports \u00b6 Elasticsearch: :9200 Kibana: :5601","title":"Overview"},{"location":"services/#summary","text":"ROCK uses a collection of open-source applications as described in this \"Services\" section. This portion of the documentation covers the basic administration of each of the major components of RockNSM.","title":"Summary"},{"location":"services/#managing-all-services","text":"If you know Linux operating systems that use systemd, the you'll be familiar with the command systemctl to manage services. ROCK provides two separate scripts that provide similar functionality, depending upon your deployment scenario. $ rockctl - service manage for a single-node instance $ rock - service manage for a multi-node instance","title":"Managing All Services"},{"location":"services/#rockctl","text":"The rockctl command is a Bash script that calls systemd to action all ROCK services. Use this tool when you're connected to a specific sensor and need to take action on the sensor services as a group. rockctl can be used to perform the following operations: $ sudo rockctl status # Report status for all ROCK services on this host stop # Stop all ROCK services on this host start # Start all ROCK services on this host restart # Restart all ROCK services on this host reset-failed # Reset failed status all ROCK services on this host # Useful for services like stenographer that start and exit","title":"rockctl"},{"location":"services/#rock","text":"The rock command is a Bash script that uses Ansible to action services across all sensors across all sensors that are part of a multi-node deployment. [ admin@rock ~ ] $ rock Commands: setup Launch TUI to configure this host for deployment tui Alias for setup ssh-config Configure hosts in inventory to use key-based auth ( multinode ) deploy Deploy selected ROCK components deploy-offline Same as deploy --offline ( Default ISO behavior ) deploy-online Same as deploy --online stop Stop all ROCK services start Start all ROCK services restart Restart all ROCK services status Report status for all ROCK services genconfig Generate default configuration based on current system destroy Destroy all ROCK data: indexes, logs, PCAP, i.e. EVERYTHING NOTE: Will not remove any services, just the data Options: --config, -c <config_yaml> Specify full path to configuration overrides --extra, -e <ansible variables> Set additional variables as key = value or YAML/JSON passed to ansible-playbook --help, -h Show this usage information --inventory, -i <inventory_path> Specify path to Ansible inventory file --limit <host> Specify host to run plays --list-hosts Outputs a list of matching hosts ; does not execute anything else --list-tags List all available tags --list-tasks List all tasks that would be executed --offline, -o Deploy ROCK using only local repos ( Default ISO behavior ) --online, -O Deploy ROCK using online repos --playbook, -p <playbook_path> Specify path to Ansible playbook file --skip-tags <tags> Only run plays and tasks whose tags do not match these values --tags, -t <tags> Only run plays and tasks tagged with these values --verbose, -v Increase verbosity of ansible-playbook","title":"rock"},{"location":"services/#managing-single-services","text":"When you need to take action on a singular service (a la carte), use systemctl as you would","title":"Managing Single Services"},{"location":"services/#published-urls-and-ports","text":"ROCK uses the lighttpd webserver to perform vhost redirects to it's web interfaces. It's configured to listen for (IPV4 only) connections over port 443 for the following:","title":"Published URLs and Ports"},{"location":"services/#urls","text":"Kibana is accessible at: https://<sensorip>/app/kibana Docket is accessible at: https://<sensorip>/app/docket/","title":"URLs"},{"location":"services/#ports","text":"Elasticsearch: :9200 Kibana: :5601","title":"Ports"},{"location":"services/docket/","text":"Docket \u00b6 Docket is a web UI that makes it easy for analysts to filter mountains of PCAP down to specific chunks in order to find the baddies . https://[sensorip]/app/docket/ Overview \u00b6 PCAP is great, but doesn't scale well. There's so much detail that it can be overwhelming to sort through. A great alternate to \"following the TCP stream\" through an ocean of packets is to use a tool like Docket that allows for easy filtering on key points such as: timeframe hosts networks ports more ... The NSM community has needed a solution like Docket for a while, and we're excited to see how it empowers the analysis process. Basic Usage \u00b6 To access Docket point to https://<sensorip>/app/docket/ . Please note the trailing slash . (This is due to Kibana being served from the same proxy and gets greedy with the URL path). Submit Request \u00b6 Once into the UI simply add your search criteria and click \"Submit\". Get PCAP \u00b6 Once the job is processed, click \"Get PCAP\" to download to your box locally. Management \u00b6 Services \u00b6 Docket requires the following services to function: lighttpd - TLS connection stenographer - tool to write / query pcap stenographer@<int> - process for each monitor interface Current status can be checked with the following commands: $ sudo systemctl status lighttpd $ sudo rockctl status Changing Lighttpd Credentials \u00b6 For the diligent (paranoid), the credentials that were initially generated at installation can be changed with the following steps: create a new shell variable, example: USER_NAME=operator append the new user to lighttpd config file: $ sudo sh -c \"echo -n '$USER_NAME:' >> /etc/lighttpd/rock-htpasswd.user\" generate new password for new user: $ sudo sh -c \"openssl passwd -apr1 >> /etc/lighttpd/rock-htpasswd.user\" Directories \u00b6 Here are some important filesystem paths that will be useful for any necessary troubleshooting efforts: PCAP Storage \u00b6 User requested PCAP jobs are saved in: /var/spool/docket In a multi-user environment this directory can fill up depending on how much space has been allocated to the /var partition. Keep this path clean to prevent issues. Python Socket \u00b6 /run/docket/","title":"Docket"},{"location":"services/docket/#docket","text":"Docket is a web UI that makes it easy for analysts to filter mountains of PCAP down to specific chunks in order to find the baddies . https://[sensorip]/app/docket/","title":"Docket"},{"location":"services/docket/#overview","text":"PCAP is great, but doesn't scale well. There's so much detail that it can be overwhelming to sort through. A great alternate to \"following the TCP stream\" through an ocean of packets is to use a tool like Docket that allows for easy filtering on key points such as: timeframe hosts networks ports more ... The NSM community has needed a solution like Docket for a while, and we're excited to see how it empowers the analysis process.","title":"Overview"},{"location":"services/docket/#basic-usage","text":"To access Docket point to https://<sensorip>/app/docket/ . Please note the trailing slash . (This is due to Kibana being served from the same proxy and gets greedy with the URL path).","title":"Basic Usage"},{"location":"services/docket/#submit-request","text":"Once into the UI simply add your search criteria and click \"Submit\".","title":"Submit Request"},{"location":"services/docket/#get-pcap","text":"Once the job is processed, click \"Get PCAP\" to download to your box locally.","title":"Get PCAP"},{"location":"services/docket/#management","text":"","title":"Management"},{"location":"services/docket/#services","text":"Docket requires the following services to function: lighttpd - TLS connection stenographer - tool to write / query pcap stenographer@<int> - process for each monitor interface Current status can be checked with the following commands: $ sudo systemctl status lighttpd $ sudo rockctl status","title":"Services"},{"location":"services/docket/#changing-lighttpd-credentials","text":"For the diligent (paranoid), the credentials that were initially generated at installation can be changed with the following steps: create a new shell variable, example: USER_NAME=operator append the new user to lighttpd config file: $ sudo sh -c \"echo -n '$USER_NAME:' >> /etc/lighttpd/rock-htpasswd.user\" generate new password for new user: $ sudo sh -c \"openssl passwd -apr1 >> /etc/lighttpd/rock-htpasswd.user\"","title":"Changing Lighttpd Credentials"},{"location":"services/docket/#directories","text":"Here are some important filesystem paths that will be useful for any necessary troubleshooting efforts:","title":"Directories"},{"location":"services/docket/#pcap-storage","text":"User requested PCAP jobs are saved in: /var/spool/docket In a multi-user environment this directory can fill up depending on how much space has been allocated to the /var partition. Keep this path clean to prevent issues.","title":"PCAP Storage"},{"location":"services/docket/#python-socket","text":"/run/docket/","title":"Python Socket"},{"location":"services/elasticsearch/","text":"Elasticsearch \u00b6 Overview \u00b6 Elasticsearch is the data storage and retrieval system in RockNSM. Elasticsearch is an \"indexed JSON document store\". Unlike other solutions, (network) events are indexed once on initial ingest, and after which you can run queries and aggregations quickly and efficiently. ROCK sends all logs preformatted in JSON, complete with human readable timestamps. This does two things: Elasticsearch compression is effectively increased since there is not two copies of the data, raw and JSON. The preformatted timestamps and JSON log data greatly increase the logging and error rate while increasing reliability of the logging infrastructure. Management \u00b6 Service \u00b6 Elasticsearch is deployed as a systemd unit, called elasticsearch.service . Normal systemd procedures apply here: sudo systemctl start elasticsearch sudo systemctl status elasticsearch sudo systemctl stop elasticsearch sudo systemctl restart elasticsearch API Access \u00b6 Elasticsearch data can be accessed via a Restful API over port 9200. Kibana is the most common way this is done, but this can also be accomplished with curl commands, such as: $ curl <SENSORIP>:9200/_cat/indices . Directories \u00b6 home: /usr/share/elasticsearch data: /data/elasticsearch/ application logs: /var/log/elasticsearch/","title":"Elasticsearch"},{"location":"services/elasticsearch/#elasticsearch","text":"","title":"Elasticsearch"},{"location":"services/elasticsearch/#overview","text":"Elasticsearch is the data storage and retrieval system in RockNSM. Elasticsearch is an \"indexed JSON document store\". Unlike other solutions, (network) events are indexed once on initial ingest, and after which you can run queries and aggregations quickly and efficiently. ROCK sends all logs preformatted in JSON, complete with human readable timestamps. This does two things: Elasticsearch compression is effectively increased since there is not two copies of the data, raw and JSON. The preformatted timestamps and JSON log data greatly increase the logging and error rate while increasing reliability of the logging infrastructure.","title":"Overview"},{"location":"services/elasticsearch/#management","text":"","title":"Management"},{"location":"services/elasticsearch/#service","text":"Elasticsearch is deployed as a systemd unit, called elasticsearch.service . Normal systemd procedures apply here: sudo systemctl start elasticsearch sudo systemctl status elasticsearch sudo systemctl stop elasticsearch sudo systemctl restart elasticsearch","title":"Service"},{"location":"services/elasticsearch/#api-access","text":"Elasticsearch data can be accessed via a Restful API over port 9200. Kibana is the most common way this is done, but this can also be accomplished with curl commands, such as: $ curl <SENSORIP>:9200/_cat/indices .","title":"API Access"},{"location":"services/elasticsearch/#directories","text":"home: /usr/share/elasticsearch data: /data/elasticsearch/ application logs: /var/log/elasticsearch/","title":"Directories"},{"location":"services/filebeat/","text":"Filebeat \u00b6 Overview \u00b6 Elastic Beats are lightweight \"data shippers\". Filebeat's role in ROCK is to do just this: ship file data to the next step in the pipeline. The following ROCK components depend on Filebeat to send their log files into the Kafka message queue: Suricata - writes alerting data into eve.json FSF - writes static file scan results to rockout.log Management \u00b6 Service \u00b6 FSF is deployed as a systemd unit, called filebeat.service . This service is configured and enabled on startup. This can be verified with either: $ sudo rockctl status $ sudo systemctl status filebeat Directories \u00b6 The configuration path for Filebeat is found at: /etc/filebeat/filebeat.yml","title":"Filebeat"},{"location":"services/filebeat/#filebeat","text":"","title":"Filebeat"},{"location":"services/filebeat/#overview","text":"Elastic Beats are lightweight \"data shippers\". Filebeat's role in ROCK is to do just this: ship file data to the next step in the pipeline. The following ROCK components depend on Filebeat to send their log files into the Kafka message queue: Suricata - writes alerting data into eve.json FSF - writes static file scan results to rockout.log","title":"Overview"},{"location":"services/filebeat/#management","text":"","title":"Management"},{"location":"services/filebeat/#service","text":"FSF is deployed as a systemd unit, called filebeat.service . This service is configured and enabled on startup. This can be verified with either: $ sudo rockctl status $ sudo systemctl status filebeat","title":"Service"},{"location":"services/filebeat/#directories","text":"The configuration path for Filebeat is found at: /etc/filebeat/filebeat.yml","title":"Directories"},{"location":"services/fsf/","text":"FSF \u00b6 FSF is included in RockNSM to provide static file analysis on filetypes of interest. Overview \u00b6 FSF works in conjuction with the file extraction framework provided by Bro . Bro can be configured to watch for specific file (mime) types, as well as establishing max file sizes that will be extracted. FSF uses a client-server model and can watch for new extracted files in the /data/fsf/ partition. Management \u00b6 Services \u00b6 FSF is deployed as a systemd unit, called fsf.service . Normal systemd procedures apply here: sudo systemctl start fsf sudo systemctl status fsf sudo systemctl stop fsf sudo systemctl restart fsf It can also be managed using the rockctl command. Directories \u00b6 Server \u00b6 /opt/fsf/fsf-server/conf/config.py - main config file /opt/fsf/fsf-server/main.py - server script Client \u00b6 /opt/fsf/fsf-client/conf/config.py - main config file /opt/fsf/fsf-client/fsf_client.py - client binary","title":"FSF"},{"location":"services/fsf/#fsf","text":"FSF is included in RockNSM to provide static file analysis on filetypes of interest.","title":"FSF"},{"location":"services/fsf/#overview","text":"FSF works in conjuction with the file extraction framework provided by Bro . Bro can be configured to watch for specific file (mime) types, as well as establishing max file sizes that will be extracted. FSF uses a client-server model and can watch for new extracted files in the /data/fsf/ partition.","title":"Overview"},{"location":"services/fsf/#management","text":"","title":"Management"},{"location":"services/fsf/#services","text":"FSF is deployed as a systemd unit, called fsf.service . Normal systemd procedures apply here: sudo systemctl start fsf sudo systemctl status fsf sudo systemctl stop fsf sudo systemctl restart fsf It can also be managed using the rockctl command.","title":"Services"},{"location":"services/fsf/#directories","text":"","title":"Directories"},{"location":"services/fsf/#server","text":"/opt/fsf/fsf-server/conf/config.py - main config file /opt/fsf/fsf-server/main.py - server script","title":"Server"},{"location":"services/fsf/#client","text":"/opt/fsf/fsf-client/conf/config.py - main config file /opt/fsf/fsf-client/fsf_client.py - client binary","title":"Client"},{"location":"services/kafka/","text":"Kafka \u00b6 Kafka is a wicked fast and reliable message queue. Overview \u00b6 Kafka solves the problem of having multiple data sources sending into the same pipeline. It acts as a staging area to allow Logstash to keep up with things. Management \u00b6 Services \u00b6 Kafka is deployed as a systemd unit, called kafka.service . Normal systemd procedures apply here: sudo systemctl start kafka sudo systemctl status kafka sudo systemctl stop kafka sudo systemctl restart kafka It can also be managed using the rockctl command. Directories \u00b6 etc/kafka/server.properties - primary config file","title":"Kafka"},{"location":"services/kafka/#kafka","text":"Kafka is a wicked fast and reliable message queue.","title":"Kafka"},{"location":"services/kafka/#overview","text":"Kafka solves the problem of having multiple data sources sending into the same pipeline. It acts as a staging area to allow Logstash to keep up with things.","title":"Overview"},{"location":"services/kafka/#management","text":"","title":"Management"},{"location":"services/kafka/#services","text":"Kafka is deployed as a systemd unit, called kafka.service . Normal systemd procedures apply here: sudo systemctl start kafka sudo systemctl status kafka sudo systemctl stop kafka sudo systemctl restart kafka It can also be managed using the rockctl command.","title":"Services"},{"location":"services/kafka/#directories","text":"etc/kafka/server.properties - primary config file","title":"Directories"},{"location":"services/kibana/","text":"Kibana \u00b6 Overview \u00b6 Kibana is the web interface used to display data inside Elasticseach . Basic Usage \u00b6 Open a web browser and visit the following url: https://<sensorip>/app/kibana . On first connection, users will be prompted for a username and password. Upon running the deploy script a random passphrase is generated in the style of XKCD . These credentials are stored in \"KIBANA_CREDS.README\" file located in the home directory of the user created at install e.g. /home/admin/KIBANA_CREDS.README . Management \u00b6 Service \u00b6 Kibana is deployed as a systemd unit, called kibana.service . Normal systemd procedures apply here: sudo systemctl start kibana sudo systemctl status kibana sudo systemctl stop kibana sudo systemctl restart kibana Directories \u00b6 Home: /usr/share/kibana Data: Stored in .kibana index in Elasticseach Application Logs: journalctl -u kibana","title":"Kibana"},{"location":"services/kibana/#kibana","text":"","title":"Kibana"},{"location":"services/kibana/#overview","text":"Kibana is the web interface used to display data inside Elasticseach .","title":"Overview"},{"location":"services/kibana/#basic-usage","text":"Open a web browser and visit the following url: https://<sensorip>/app/kibana . On first connection, users will be prompted for a username and password. Upon running the deploy script a random passphrase is generated in the style of XKCD . These credentials are stored in \"KIBANA_CREDS.README\" file located in the home directory of the user created at install e.g. /home/admin/KIBANA_CREDS.README .","title":"Basic Usage"},{"location":"services/kibana/#management","text":"","title":"Management"},{"location":"services/kibana/#service","text":"Kibana is deployed as a systemd unit, called kibana.service . Normal systemd procedures apply here: sudo systemctl start kibana sudo systemctl status kibana sudo systemctl stop kibana sudo systemctl restart kibana","title":"Service"},{"location":"services/kibana/#directories","text":"Home: /usr/share/kibana Data: Stored in .kibana index in Elasticseach Application Logs: journalctl -u kibana","title":"Directories"},{"location":"services/logstash/","text":"Logstash \u00b6 Logstash is part of the Elastic Stack that performs log file filtering and enrichment. Management \u00b6 Services \u00b6 Logstash is deployed as a systemd unit, called logstash.service . Normal systemd procedures apply here: sudo systemctl start logstash sudo systemctl status logstash sudo systemctl stop logstash sudo systemctl restart logstash It can also be managed using the rockctl command. Directories \u00b6 /etc/logstash/ - main config path /etc/logstash/conf.d - ROCK specific config /var/lib/logstash - data path","title":"Logstash"},{"location":"services/logstash/#logstash","text":"Logstash is part of the Elastic Stack that performs log file filtering and enrichment.","title":"Logstash"},{"location":"services/logstash/#management","text":"","title":"Management"},{"location":"services/logstash/#services","text":"Logstash is deployed as a systemd unit, called logstash.service . Normal systemd procedures apply here: sudo systemctl start logstash sudo systemctl status logstash sudo systemctl stop logstash sudo systemctl restart logstash It can also be managed using the rockctl command.","title":"Services"},{"location":"services/logstash/#directories","text":"/etc/logstash/ - main config path /etc/logstash/conf.d - ROCK specific config /var/lib/logstash - data path","title":"Directories"},{"location":"services/stenographer/","text":"Stenographer \u00b6 ROCK uses Stenographer for full packet capture. Among other features, it provides the following advantages over other solutions: it's fast manages disk space will fill it's disk partition to 90% then start to overwrite oldest data forward Management \u00b6 Systemd \u00b6 Stenographer is deployed as a systemd unit, called stenographer.service . Normal systemd procedures apply here: sudo systemctl start stenographer sudo systemctl status stenographer sudo systemctl stop stenographer sudo systemctl restart stenographer Rockctl \u00b6 It can also be managed using the rockctl command. Multiple Interfaces \u00b6 It's important to note that Stenographer will have a (main) parent process, and a child process for every interface that it uses to capture packets. ex: STENOGRAPHER: Active: active ( exited ) since Mon 2019 -01-28 22 :51:47 UTC ; 1 weeks 0 days ago STENOGRAPHER@EM1: Active: active ( running ) since Mon 2019 -01-28 22 :51:47 UTC ; 1 weeks 0 days ago In order to restart all Stenographer processes, a wildcard can be used: sudo sytemctl restart stenographer* Directories \u00b6 Stenographer is great at managing it's own disk space, but that doesn't cut it when it's sharing the same mount point as Bro, Suricata , and other tools that generate data in ROCK. Best practice would be to create a /data/stenographer partition in order to prevent limited operations.","title":"Stenographer"},{"location":"services/stenographer/#stenographer","text":"ROCK uses Stenographer for full packet capture. Among other features, it provides the following advantages over other solutions: it's fast manages disk space will fill it's disk partition to 90% then start to overwrite oldest data forward","title":"Stenographer"},{"location":"services/stenographer/#management","text":"","title":"Management"},{"location":"services/stenographer/#systemd","text":"Stenographer is deployed as a systemd unit, called stenographer.service . Normal systemd procedures apply here: sudo systemctl start stenographer sudo systemctl status stenographer sudo systemctl stop stenographer sudo systemctl restart stenographer","title":"Systemd"},{"location":"services/stenographer/#rockctl","text":"It can also be managed using the rockctl command.","title":"Rockctl"},{"location":"services/stenographer/#multiple-interfaces","text":"It's important to note that Stenographer will have a (main) parent process, and a child process for every interface that it uses to capture packets. ex: STENOGRAPHER: Active: active ( exited ) since Mon 2019 -01-28 22 :51:47 UTC ; 1 weeks 0 days ago STENOGRAPHER@EM1: Active: active ( running ) since Mon 2019 -01-28 22 :51:47 UTC ; 1 weeks 0 days ago In order to restart all Stenographer processes, a wildcard can be used: sudo sytemctl restart stenographer*","title":"Multiple Interfaces"},{"location":"services/stenographer/#directories","text":"Stenographer is great at managing it's own disk space, but that doesn't cut it when it's sharing the same mount point as Bro, Suricata , and other tools that generate data in ROCK. Best practice would be to create a /data/stenographer partition in order to prevent limited operations.","title":"Directories"},{"location":"services/suricata/","text":"Suricata \u00b6 Intrusion Detection Systems (IDS) are a great way to quickly alert on known bad. Alerts are triggered when a packet matches a defined pattern or signature . Suricata is the IDS / Alerting tool of choice for RockNSM. It provides a lot of features not available in our previous option. Most importantly, Suricata offers: multi-threading capability active development community and frequent feature additions & project momentum Service Management \u00b6 Suricata is deployed as a systemd unit called suricata.service . Normal systemd procedures apply here. It can also be managed using the rockctl command using the same syntax: sudo systemctl start suricata sudo systemctl status suricata sudo systemctl stop suricata sudo systemctl restart suricata The default ROCK configuration has the Suricata service enabled on startup. Notable Files / Directories \u00b6 /etc/suricata/ - main configuration path /var/lib/suricata/ - primary rule path Updating Rules \u00b6 The newest versions of Suricata come with the suricata-update command to manange and update rulesets. The official documentation is found here . Enabling Feeds \u00b6 Suricata Update is a Python module and is automatically bundled with Suricata starting with version 4.1. While it does have documentation, it\u2019s helpful to have a practical example. One of the awesome features with Suricata Update is it comes with a pre-configured list of signature feeds out of the box, both free and paid. It makes it very simple to enabled paid feeds. To view the list of available feeds, login to your RockNSM system and run: $ sudo suricata-update list-sources This will return something similar to the following: Name : oisf/trafficid Vendor : OISF Summary : Suricata Traffic ID ruleset License : MIT Name : et/open Vendor : Proofpoint Summary : Emerging Threats Open Ruleset License : MIT Name : scwx/security Vendor : Secureworks Summary : Secureworks suricata-security ruleset. License : Commercial Parameters : secret-code Subscription : https://www.secureworks.com/contact/ (Please reference CTU Countermeasures) Name : scwx/malware Vendor : Secureworks Summary : Secureworks suricata-malware ruleset. License : Commercial Parameters : secret-code Subscription : https://www.secureworks.com/contact/ (Please reference CTU Countermeasures) Name : et/pro Vendor : Proofpoint Summary : Emerging Threats Pro Ruleset License : Commercial Replaces : et/open Parameters : secret-code Subscription : https://www.proofpoint.com/us/threat-insight/et-pro-ruleset Name : ptresearch/attackdetection Vendor : Positive Technologies Summary : Positive Technologies Attack Detection Team ruleset License : Custom Name : sslbl/ssl-fp-blacklist Vendor : Abuse.ch Summary : Abuse.ch SSL Blacklist License : Non-Commercial Name : tgreen/hunting Vendor : tgreen Summary : Heuristic ruleset for hunting. Focus on anomaly detection and showcasing latest engine features, not performance. License : GPLv3 Name : etnetera/aggressive Vendor : Etnetera a.s. Summary : Etnetera aggressive IP blacklist License : MIT Without any additional configuration, suricata-update will automatically pull in the et/open ruleset. You can disable this ruleset if you desire. Now, if you are a subscriber to et/pro or another included ruleset that requires an access code (sometimes referred to as an \u201coinkcode\u201d in Snort parlance), you can pass that on the command line or suricata-update will prompt you. suricata-update enable-source et/pro secret-code=xxxxxxxxxxxxxxxx Manipulating Individual Rules \u00b6 Often times, we want to turn off specific rules\u200a\u2014\u200amaybe they\u2019re too noisy for our network, or corporate policy doesn\u2019t concern with browser extensions on BYOD systems. Again, suricata-update makes our life easy on our RockNSM sensors. # Elevate to a root shell and go to Suricata dir sudo -s cd /etc/suricata # Generate default suricata-update configs suricata-update --dump-sample-configs This command will generate six default files: update.yaml - the suricata-update config file enable.conf - config to enable rules that are usually disabled disable.conf - config to disable rules that are usually enabled modify.conf - use regex to modify rules drop.conf - change rules from alert to drop, not used in RockNSM threshold.in - set thresholds to limit too-frequent firing of given alerts To disable the noisy rule above, we just need to specify its signature ID (e.g. alert.signature_id). Open disable.conf and add the following line: # Disable invalid timestamp rule (sid: 2210044) 2210044 We could alternatively specify the rule using regular expressions: # Disable all SURICATA STREAM alert rules re:^alert.*SURICATA STREAM Next, just run suricata-update . Note, you want to ensure that suricata-update runs as the same user as the suricata service. On RockNSM, this is the suricata system user. This should really only be necessary the first time suricata-update is run, to ensure that when Suricata runs it can read the rules. Near the end of the run, you\u2019ll see a summary how many rules were loaded, disabled, enabled, modified, dropped, and some other stats. sudo -u suricata -g suricata suricata-update ... 27/2/2019 -- 00:08:39 - <Info> -- Loaded 29733 rules. 27/2/2019 -- 00:08:40 - <Info> -- Disabled 68 rules. 27/2/2019 -- 00:08:40 - <Info> -- Enabled 0 rules. 27/2/2019 -- 00:08:40 - <Info> -- Modified 0 rules. 27/2/2019 -- 00:08:40 - <Info> -- Dropped 0 rules. 27/2/2019 -- 00:08:41 - <Info> -- Enabled 188 rules for flowbit dependencies. 27/2/2019 -- 00:08:41 - <Info> -- Backing up current rules. 27/2/2019 -- 00:08:45 - <Info> -- Writing rules to /var/lib/suricata/rules/suricata.rules: total: 29733; enabled: 22272; added: 4; removed 19; modified: 1215 You can sanity check that it worked by checking the output for the signature you disabled with grep. You could also search using the same regex as before! If you want to match the regex pattern, be sure to search for a line starting with a # followed by a single space, as this is how the rule is commented out. If the disable configuration worked, you\u2019ll see the rule, but it will be commented out. sudo grep 2210044 /var/lib/suricata/rules/suricata.rules # alert tcp any any -> any any (msg:\"SURICATA STREAM Packet with invalid timestamp\"; stream-event:pkt_invalid_timestamp; classtype:protocol-command-decode; sid:2210044; rev:2;) To check for a regex you could do this. sudo grep '^# alert.*SURICATA STREAM' /var/lib/suricata/rules/suricata.rules ... (a whole bunch of rules match this) Local Rule Management \u00b6 Suricata Update lets you manage local rules using the same process above. In the update.yaml it defaults to loading all rules in the /etc/suricata/rules directory. You could add some local site-specific directory, as well. Suricata Update will parse each of these rules and apply the same operations that you configured, as detailed above. Automating It \u00b6 RockNSM automatically will run your suricata-update process once per day. This is done using crond in /etc/cron.d/rocknsm_suricata-update every day at noon UTC (which is the default and recommended RockNSM sensor time zone).","title":"Suricata"},{"location":"services/suricata/#suricata","text":"Intrusion Detection Systems (IDS) are a great way to quickly alert on known bad. Alerts are triggered when a packet matches a defined pattern or signature . Suricata is the IDS / Alerting tool of choice for RockNSM. It provides a lot of features not available in our previous option. Most importantly, Suricata offers: multi-threading capability active development community and frequent feature additions & project momentum","title":"Suricata"},{"location":"services/suricata/#service-management","text":"Suricata is deployed as a systemd unit called suricata.service . Normal systemd procedures apply here. It can also be managed using the rockctl command using the same syntax: sudo systemctl start suricata sudo systemctl status suricata sudo systemctl stop suricata sudo systemctl restart suricata The default ROCK configuration has the Suricata service enabled on startup.","title":"Service Management"},{"location":"services/suricata/#notable-files-directories","text":"/etc/suricata/ - main configuration path /var/lib/suricata/ - primary rule path","title":"Notable Files / Directories"},{"location":"services/suricata/#updating-rules","text":"The newest versions of Suricata come with the suricata-update command to manange and update rulesets. The official documentation is found here .","title":"Updating Rules"},{"location":"services/suricata/#enabling-feeds","text":"Suricata Update is a Python module and is automatically bundled with Suricata starting with version 4.1. While it does have documentation, it\u2019s helpful to have a practical example. One of the awesome features with Suricata Update is it comes with a pre-configured list of signature feeds out of the box, both free and paid. It makes it very simple to enabled paid feeds. To view the list of available feeds, login to your RockNSM system and run: $ sudo suricata-update list-sources This will return something similar to the following: Name : oisf/trafficid Vendor : OISF Summary : Suricata Traffic ID ruleset License : MIT Name : et/open Vendor : Proofpoint Summary : Emerging Threats Open Ruleset License : MIT Name : scwx/security Vendor : Secureworks Summary : Secureworks suricata-security ruleset. License : Commercial Parameters : secret-code Subscription : https://www.secureworks.com/contact/ (Please reference CTU Countermeasures) Name : scwx/malware Vendor : Secureworks Summary : Secureworks suricata-malware ruleset. License : Commercial Parameters : secret-code Subscription : https://www.secureworks.com/contact/ (Please reference CTU Countermeasures) Name : et/pro Vendor : Proofpoint Summary : Emerging Threats Pro Ruleset License : Commercial Replaces : et/open Parameters : secret-code Subscription : https://www.proofpoint.com/us/threat-insight/et-pro-ruleset Name : ptresearch/attackdetection Vendor : Positive Technologies Summary : Positive Technologies Attack Detection Team ruleset License : Custom Name : sslbl/ssl-fp-blacklist Vendor : Abuse.ch Summary : Abuse.ch SSL Blacklist License : Non-Commercial Name : tgreen/hunting Vendor : tgreen Summary : Heuristic ruleset for hunting. Focus on anomaly detection and showcasing latest engine features, not performance. License : GPLv3 Name : etnetera/aggressive Vendor : Etnetera a.s. Summary : Etnetera aggressive IP blacklist License : MIT Without any additional configuration, suricata-update will automatically pull in the et/open ruleset. You can disable this ruleset if you desire. Now, if you are a subscriber to et/pro or another included ruleset that requires an access code (sometimes referred to as an \u201coinkcode\u201d in Snort parlance), you can pass that on the command line or suricata-update will prompt you. suricata-update enable-source et/pro secret-code=xxxxxxxxxxxxxxxx","title":"Enabling Feeds"},{"location":"services/suricata/#manipulating-individual-rules","text":"Often times, we want to turn off specific rules\u200a\u2014\u200amaybe they\u2019re too noisy for our network, or corporate policy doesn\u2019t concern with browser extensions on BYOD systems. Again, suricata-update makes our life easy on our RockNSM sensors. # Elevate to a root shell and go to Suricata dir sudo -s cd /etc/suricata # Generate default suricata-update configs suricata-update --dump-sample-configs This command will generate six default files: update.yaml - the suricata-update config file enable.conf - config to enable rules that are usually disabled disable.conf - config to disable rules that are usually enabled modify.conf - use regex to modify rules drop.conf - change rules from alert to drop, not used in RockNSM threshold.in - set thresholds to limit too-frequent firing of given alerts To disable the noisy rule above, we just need to specify its signature ID (e.g. alert.signature_id). Open disable.conf and add the following line: # Disable invalid timestamp rule (sid: 2210044) 2210044 We could alternatively specify the rule using regular expressions: # Disable all SURICATA STREAM alert rules re:^alert.*SURICATA STREAM Next, just run suricata-update . Note, you want to ensure that suricata-update runs as the same user as the suricata service. On RockNSM, this is the suricata system user. This should really only be necessary the first time suricata-update is run, to ensure that when Suricata runs it can read the rules. Near the end of the run, you\u2019ll see a summary how many rules were loaded, disabled, enabled, modified, dropped, and some other stats. sudo -u suricata -g suricata suricata-update ... 27/2/2019 -- 00:08:39 - <Info> -- Loaded 29733 rules. 27/2/2019 -- 00:08:40 - <Info> -- Disabled 68 rules. 27/2/2019 -- 00:08:40 - <Info> -- Enabled 0 rules. 27/2/2019 -- 00:08:40 - <Info> -- Modified 0 rules. 27/2/2019 -- 00:08:40 - <Info> -- Dropped 0 rules. 27/2/2019 -- 00:08:41 - <Info> -- Enabled 188 rules for flowbit dependencies. 27/2/2019 -- 00:08:41 - <Info> -- Backing up current rules. 27/2/2019 -- 00:08:45 - <Info> -- Writing rules to /var/lib/suricata/rules/suricata.rules: total: 29733; enabled: 22272; added: 4; removed 19; modified: 1215 You can sanity check that it worked by checking the output for the signature you disabled with grep. You could also search using the same regex as before! If you want to match the regex pattern, be sure to search for a line starting with a # followed by a single space, as this is how the rule is commented out. If the disable configuration worked, you\u2019ll see the rule, but it will be commented out. sudo grep 2210044 /var/lib/suricata/rules/suricata.rules # alert tcp any any -> any any (msg:\"SURICATA STREAM Packet with invalid timestamp\"; stream-event:pkt_invalid_timestamp; classtype:protocol-command-decode; sid:2210044; rev:2;) To check for a regex you could do this. sudo grep '^# alert.*SURICATA STREAM' /var/lib/suricata/rules/suricata.rules ... (a whole bunch of rules match this)","title":"Manipulating Individual Rules"},{"location":"services/suricata/#local-rule-management","text":"Suricata Update lets you manage local rules using the same process above. In the update.yaml it defaults to loading all rules in the /etc/suricata/rules directory. You could add some local site-specific directory, as well. Suricata Update will parse each of these rules and apply the same operations that you configured, as detailed above.","title":"Local Rule Management"},{"location":"services/suricata/#automating-it","text":"RockNSM automatically will run your suricata-update process once per day. This is done using crond in /etc/cron.d/rocknsm_suricata-update every day at noon UTC (which is the default and recommended RockNSM sensor time zone).","title":"Automating It"},{"location":"services/zeek/","text":"Zeek \u00b6 Note: While \"Zeek\" is the new name of the project, directories, service files, and binaries still (for now) retain the \"bro\" name. Overview \u00b6 Zeek (the artist formerly known as Bro) is used to provide network protocol analysis within ROCK. It is extremely customizable, and it is encouraged that you take advantage of this. When deploying custom Zeek scripts, please be sure to store them under a subdirectory of /usr/share/bro/site/scripts/ . We can't guarantee that your customizations won't be overwritten by Ansible if you don't follow this pattern. Management \u00b6 Service \u00b6 Zeek is deployed as a systemd unit, called bro.service . Normal systemd procedures apply here: sudo systemctl start bro sudo systemctl status bro sudo systemctl stop bro sudo systemctl restart bro The broctl command is now an alias. Using this alias prevents dangerous permission changes caused by running the real broctl binary with sudo. The only safe way otherwise to run broctl is to execute it as the bro user and bro group as such: sudo -u bro -g bro /usr/bin/broctl Directories \u00b6 Home /usr/share/bro/ Data /data/bro/logs/current/{stream_name.log} Application Logs /data/bro/logs/current/{stdout.log, stderr.log} Note: By default, Zeek will write ASCII logs to the data path above AND write JSON directly to Kafka. In general, you will be accessing the Bro data from Elasticsearch via Kibana .","title":"Zeek"},{"location":"services/zeek/#zeek","text":"Note: While \"Zeek\" is the new name of the project, directories, service files, and binaries still (for now) retain the \"bro\" name.","title":"Zeek"},{"location":"services/zeek/#overview","text":"Zeek (the artist formerly known as Bro) is used to provide network protocol analysis within ROCK. It is extremely customizable, and it is encouraged that you take advantage of this. When deploying custom Zeek scripts, please be sure to store them under a subdirectory of /usr/share/bro/site/scripts/ . We can't guarantee that your customizations won't be overwritten by Ansible if you don't follow this pattern.","title":"Overview"},{"location":"services/zeek/#management","text":"","title":"Management"},{"location":"services/zeek/#service","text":"Zeek is deployed as a systemd unit, called bro.service . Normal systemd procedures apply here: sudo systemctl start bro sudo systemctl status bro sudo systemctl stop bro sudo systemctl restart bro The broctl command is now an alias. Using this alias prevents dangerous permission changes caused by running the real broctl binary with sudo. The only safe way otherwise to run broctl is to execute it as the bro user and bro group as such: sudo -u bro -g bro /usr/bin/broctl","title":"Service"},{"location":"services/zeek/#directories","text":"Home /usr/share/bro/ Data /data/bro/logs/current/{stream_name.log} Application Logs /data/bro/logs/current/{stdout.log, stderr.log} Note: By default, Zeek will write ASCII logs to the data path above AND write JSON directly to Kafka. In general, you will be accessing the Bro data from Elasticsearch via Kibana .","title":"Directories"},{"location":"usage/","text":"Basic Usage \u00b6 Key Interfaces \u00b6 Kibana - https://localhost \u00b6 :warning: We are aware of an issue with macOS Catalina and the most current version of Chrome browser that prevents Chrome from allowing self-signed TLS certificates. We are looking for an answer and will update when we find that. This does not affect Safari or Firefox or other operating systems. As a workaround, you can manually add and Always Trust the RockNSM TLS certificate to your macOS keychain via Keychain Access and restart Chrome. The generated credentials are in the home directory of the user created at install: ~/KIBANA_CREDS.README Docket - https://localhost/app/docket/ \u00b6 Docket - web interface for pulling PCAP from the sensor (must be enabled in config) localhost or IP of the management interface of the box Functions Checks \u00b6 Cluster Health \u00b6 Check to see that the ES cluster says it's green: curl -s localhost:9200/_cluster/health?pretty Document Check \u00b6 See how many documents are in the indexes. The count should be non-zero: curl -s localhost:9200/_all/_count?pretty Testing with PCAP \u00b6 You can fire some traffic across the sensor at this point to see if it's collecting. This requires that you upload your own test PCAP to the box. PCAP is typically huge, so if you don't have any just lying around, here's a quick test: Download a small test file from the folks who brought us tcpreplay here : curl -LO https://s3.amazonaws.com/tcpreplay-pcap-files/smallFlows.pcap Replay the PCAP file across your monitor interface : sudo tcpreplay -i [your-monitor-interface] /path/to/smallflow.pcap After a few moments, the document count should go up. This can again be validated with: curl -s localhost:9200/_all/_count?pretty You should have plain text bro logs showing up in /data/bro/logs/current/: ls -ltr /data/bro/logs/current/ Rockctl \u00b6 The basic service management functions are accomplished with: sudo rockctl status - get the status of ROCK services sudo rockctl start - start ROCK services sudo rockctl stop - stop ROCK services sudo rockctl reset-failed - clear the failed states of services","title":"Basic Operation"},{"location":"usage/#basic-usage","text":"","title":"Basic Usage"},{"location":"usage/#key-interfaces","text":"","title":"Key Interfaces"},{"location":"usage/#kibana-httpslocalhost","text":":warning: We are aware of an issue with macOS Catalina and the most current version of Chrome browser that prevents Chrome from allowing self-signed TLS certificates. We are looking for an answer and will update when we find that. This does not affect Safari or Firefox or other operating systems. As a workaround, you can manually add and Always Trust the RockNSM TLS certificate to your macOS keychain via Keychain Access and restart Chrome. The generated credentials are in the home directory of the user created at install: ~/KIBANA_CREDS.README","title":"Kibana - https://localhost"},{"location":"usage/#docket-httpslocalhostappdocket","text":"Docket - web interface for pulling PCAP from the sensor (must be enabled in config) localhost or IP of the management interface of the box","title":"Docket - https://localhost/app/docket/"},{"location":"usage/#functions-checks","text":"","title":"Functions Checks"},{"location":"usage/#cluster-health","text":"Check to see that the ES cluster says it's green: curl -s localhost:9200/_cluster/health?pretty","title":"Cluster Health"},{"location":"usage/#document-check","text":"See how many documents are in the indexes. The count should be non-zero: curl -s localhost:9200/_all/_count?pretty","title":"Document Check"},{"location":"usage/#testing-with-pcap","text":"You can fire some traffic across the sensor at this point to see if it's collecting. This requires that you upload your own test PCAP to the box. PCAP is typically huge, so if you don't have any just lying around, here's a quick test: Download a small test file from the folks who brought us tcpreplay here : curl -LO https://s3.amazonaws.com/tcpreplay-pcap-files/smallFlows.pcap Replay the PCAP file across your monitor interface : sudo tcpreplay -i [your-monitor-interface] /path/to/smallflow.pcap After a few moments, the document count should go up. This can again be validated with: curl -s localhost:9200/_all/_count?pretty You should have plain text bro logs showing up in /data/bro/logs/current/: ls -ltr /data/bro/logs/current/","title":"Testing with PCAP"},{"location":"usage/#rockctl","text":"The basic service management functions are accomplished with: sudo rockctl status - get the status of ROCK services sudo rockctl start - start ROCK services sudo rockctl stop - stop ROCK services sudo rockctl reset-failed - clear the failed states of services","title":"Rockctl"},{"location":"usage/support/","text":"Support Guide \u00b6 This section aims to smooth out the most frequent issues new users will run into. Suricata Service Fails to Start \u00b6 This can occur after successful installation of the ROCK sensor. To identify if this is an issue, run sudo rockctl status and you'll see SURICATA: Active: failed (Result: exit-code) since Mon 2020-01-06 01:00:57 UTC; 40min ago To validate, run run sudo journalctl -u suricata and look for the MemoryDenyWriteExecute , meaning Suricata is using more RAM than is available. We need to tamp 'er down. sudo journalctl -u suricata -- Logs begin at Sun 2020-01-05 22:14:04 UTC, end at Mon 2020-01-06 01:42:58 UTC. -- Jan 05 22:57:54 rock-1.rock.lan systemd[1]: [/usr/lib/systemd/system/suricata.service:17] Unknown lvalue 'MemoryDenyWriteExecute' in section 'Service' Jan 05 22:57:54 rock-1.rock.lan systemd[1]: [/usr/lib/systemd/system/suricata.service:18] Unknown lvalue 'LockPersonality' in section 'Service' Jan 05 22:57:54 rock-1.rock.lan systemd[1]: [/usr/lib/systemd/system/suricata.service:19] Unknown lvalue 'ProtectControlGroups' in section 'Service' Jan 05 22:57:54 rock-1.rock.lan systemd[1]: [/usr/lib/systemd/system/suricata.service:20] Unknown lvalue 'ProtectKernelModules' in section 'Service' Jan 05 22:58:05 rock-1.rock.lan systemd[1]: Started Suricata Intrusion Detection Service. To fix this issue, identify the non-management interface that is connected by using ip link and looking for the interface that has BROADCAST and state UP . In this example, ens192f1 is UP ( ens193 is the management interface). ip link 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens193: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00:50:56:9f:c1:4e brd ff:ff:ff:ff:ff:ff 3: ens192f0: <NO-CARRIER,BROADCAST,MULTICAST,PROMISC,UP> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000 link/ether f8:f2:1e:34:0f:40 brd ff:ff:ff:ff:ff:ff 4: ens192f1: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether f8:f2:1e:34:0f:41 brd ff:ff:ff:ff:ff:ff 5: ens256f0: <NO-CARRIER,BROADCAST,MULTICAST,PROMISC,UP> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000 link/ether f8:f2:1e:34:0a:80 brd ff:ff:ff:ff:ff:ff 6: ens256f1: <NO-CARRIER,BROADCAST,MULTICAST,PROMISC,UP> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000 link/ether f8:f2:1e:34:0a:81 brd ff:ff:ff:ff:ff:ff sudo vi /etc/suricata/rocknsm-overrides.yaml and comment out everything under the af-packet heading with the exception of the connected non-management interface and setting the threads from auto to 12 (in our example, but you may want to use a different number based on your environment). Example: af-packet: # - interface: ens192f0 # #threads: auto # cluster-id: 99 # cluster-type: cluster_flow # defrag: true # use-mmap: true # mmap-locked: true # #rollover: true # tpacket-v3: true # use-emergency-flush: true - interface: ens192f1 threads: 12 cluster-id: 98 cluster-type: cluster_flow defrag: true use-mmap: true mmap-locked: true #rollover: true tpacket-v3: true use-emergency-flush: true # - interface: ens256f1 # #threads: auto # cluster-id: 97 # cluster-type: cluster_flow # defrag: true # use-mmap: true # mmap-locked: true # #rollover: true # tpacket-v3: true # use-emergency-flush: true # - interface: ens256f0 # #threads: auto # cluster-id: 96 # cluster-type: cluster_flow # defrag: true # use-mmap: true # mmap-locked: true # #rollover: true # tpacket-v3: true # use-emergency-flush: true Afterwards, restart sudo rockctl status and verify everything is started: SURICATA: Active: active (running) since Mon 2020-01-06 01:55:37 UTC; 1min 28s ago Autodetect Assumptions \u00b6 When writing the scripts to generate default values, we had to make some assumptions. The defaults are generated according to these assumptions and should generally work if your sensor aligns with them. That said, these assumptions will give you a working sensor, but may need some love for higher performance. If you cannot meet these assumptions, look at the indicated configuration variables in /etc/rocknsm/config.yml for workaround approaches (with impact on performance). TIP: We assume that any interface that does not have a default route will be used for collection. Each sensor application will be configured accordingly. WARNING : This so far has been the number one problem with a fresh install for beta testers!! Check your interface configuration!! Two Network Interfaces: a management interface with a default route an interface without a default route (defined by rock_monifs ) You have mounted your largest storage volume(s) under /data/ (defined by rock_data_dir ) Your hostname (FQDN) is defined in the playbooks/inventory/all-in-one.ini file You allow management via SSH from any network (defined by rock_mgmt_nets ) You wish to use Zeek, Suricata, Stenographer (disabled by default) and the whole data pipeline. (See with_* options) If installed via ISO, you will perform an offline install, else we assume online (defined by rock_online_install ) Zeek will use half of your CPU resources, up to 8 CPUs We will continue to add more support information as the userbase grows. Deployment Script \u00b6 If you find the deployment is failing, the script can be run with very verbose output. This example will write the output to a file for review: DEBUG=1 ./deploy_rock.sh | tee /tmp/deploy_rock.log Log Timestamps \u00b6 UTC is generally preferred for logging data as the timestamps from anywhere in the world will have a proper order without calculating offsets. That said, Kibana will present the zeek logs according to your timezone (as set in the browser). The logs themselves (i.e. in /data/bro/logs/) log in epoch time and will be written in UTC regardless of the system timezone. Zeek includes a utility for parsing these on the command line called bro-cut . It can be used to print human-readable timestamps in either the local sensor timezone or UTC. You can also give it a custom format string to specify what you'd like displayed.","title":"Support"},{"location":"usage/support/#support-guide","text":"This section aims to smooth out the most frequent issues new users will run into.","title":"Support Guide"},{"location":"usage/support/#suricata-service-fails-to-start","text":"This can occur after successful installation of the ROCK sensor. To identify if this is an issue, run sudo rockctl status and you'll see SURICATA: Active: failed (Result: exit-code) since Mon 2020-01-06 01:00:57 UTC; 40min ago To validate, run run sudo journalctl -u suricata and look for the MemoryDenyWriteExecute , meaning Suricata is using more RAM than is available. We need to tamp 'er down. sudo journalctl -u suricata -- Logs begin at Sun 2020-01-05 22:14:04 UTC, end at Mon 2020-01-06 01:42:58 UTC. -- Jan 05 22:57:54 rock-1.rock.lan systemd[1]: [/usr/lib/systemd/system/suricata.service:17] Unknown lvalue 'MemoryDenyWriteExecute' in section 'Service' Jan 05 22:57:54 rock-1.rock.lan systemd[1]: [/usr/lib/systemd/system/suricata.service:18] Unknown lvalue 'LockPersonality' in section 'Service' Jan 05 22:57:54 rock-1.rock.lan systemd[1]: [/usr/lib/systemd/system/suricata.service:19] Unknown lvalue 'ProtectControlGroups' in section 'Service' Jan 05 22:57:54 rock-1.rock.lan systemd[1]: [/usr/lib/systemd/system/suricata.service:20] Unknown lvalue 'ProtectKernelModules' in section 'Service' Jan 05 22:58:05 rock-1.rock.lan systemd[1]: Started Suricata Intrusion Detection Service. To fix this issue, identify the non-management interface that is connected by using ip link and looking for the interface that has BROADCAST and state UP . In this example, ens192f1 is UP ( ens193 is the management interface). ip link 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens193: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00:50:56:9f:c1:4e brd ff:ff:ff:ff:ff:ff 3: ens192f0: <NO-CARRIER,BROADCAST,MULTICAST,PROMISC,UP> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000 link/ether f8:f2:1e:34:0f:40 brd ff:ff:ff:ff:ff:ff 4: ens192f1: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether f8:f2:1e:34:0f:41 brd ff:ff:ff:ff:ff:ff 5: ens256f0: <NO-CARRIER,BROADCAST,MULTICAST,PROMISC,UP> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000 link/ether f8:f2:1e:34:0a:80 brd ff:ff:ff:ff:ff:ff 6: ens256f1: <NO-CARRIER,BROADCAST,MULTICAST,PROMISC,UP> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000 link/ether f8:f2:1e:34:0a:81 brd ff:ff:ff:ff:ff:ff sudo vi /etc/suricata/rocknsm-overrides.yaml and comment out everything under the af-packet heading with the exception of the connected non-management interface and setting the threads from auto to 12 (in our example, but you may want to use a different number based on your environment). Example: af-packet: # - interface: ens192f0 # #threads: auto # cluster-id: 99 # cluster-type: cluster_flow # defrag: true # use-mmap: true # mmap-locked: true # #rollover: true # tpacket-v3: true # use-emergency-flush: true - interface: ens192f1 threads: 12 cluster-id: 98 cluster-type: cluster_flow defrag: true use-mmap: true mmap-locked: true #rollover: true tpacket-v3: true use-emergency-flush: true # - interface: ens256f1 # #threads: auto # cluster-id: 97 # cluster-type: cluster_flow # defrag: true # use-mmap: true # mmap-locked: true # #rollover: true # tpacket-v3: true # use-emergency-flush: true # - interface: ens256f0 # #threads: auto # cluster-id: 96 # cluster-type: cluster_flow # defrag: true # use-mmap: true # mmap-locked: true # #rollover: true # tpacket-v3: true # use-emergency-flush: true Afterwards, restart sudo rockctl status and verify everything is started: SURICATA: Active: active (running) since Mon 2020-01-06 01:55:37 UTC; 1min 28s ago","title":"Suricata Service Fails to Start"},{"location":"usage/support/#autodetect-assumptions","text":"When writing the scripts to generate default values, we had to make some assumptions. The defaults are generated according to these assumptions and should generally work if your sensor aligns with them. That said, these assumptions will give you a working sensor, but may need some love for higher performance. If you cannot meet these assumptions, look at the indicated configuration variables in /etc/rocknsm/config.yml for workaround approaches (with impact on performance). TIP: We assume that any interface that does not have a default route will be used for collection. Each sensor application will be configured accordingly. WARNING : This so far has been the number one problem with a fresh install for beta testers!! Check your interface configuration!! Two Network Interfaces: a management interface with a default route an interface without a default route (defined by rock_monifs ) You have mounted your largest storage volume(s) under /data/ (defined by rock_data_dir ) Your hostname (FQDN) is defined in the playbooks/inventory/all-in-one.ini file You allow management via SSH from any network (defined by rock_mgmt_nets ) You wish to use Zeek, Suricata, Stenographer (disabled by default) and the whole data pipeline. (See with_* options) If installed via ISO, you will perform an offline install, else we assume online (defined by rock_online_install ) Zeek will use half of your CPU resources, up to 8 CPUs We will continue to add more support information as the userbase grows.","title":"Autodetect Assumptions"},{"location":"usage/support/#deployment-script","text":"If you find the deployment is failing, the script can be run with very verbose output. This example will write the output to a file for review: DEBUG=1 ./deploy_rock.sh | tee /tmp/deploy_rock.log","title":"Deployment Script"},{"location":"usage/support/#log-timestamps","text":"UTC is generally preferred for logging data as the timestamps from anywhere in the world will have a proper order without calculating offsets. That said, Kibana will present the zeek logs according to your timezone (as set in the browser). The logs themselves (i.e. in /data/bro/logs/) log in epoch time and will be written in UTC regardless of the system timezone. Zeek includes a utility for parsing these on the command line called bro-cut . It can be used to print human-readable timestamps in either the local sensor timezone or UTC. You can also give it a custom format string to specify what you'd like displayed.","title":"Log Timestamps"}]}